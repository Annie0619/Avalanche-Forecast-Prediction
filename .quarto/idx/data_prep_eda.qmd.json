{"title":"Data Preparation and EDA","markdown":{"yaml":{"title":"Data Preparation and EDA","engine":"knitr"},"headingText":"read in the data","containsRefs":false,"markdown":"\n\n```{r}\nlibrary(reticulate)\nuse_condaenv(\"ds4i-mixed\", required = TRUE)\npy_config()\n```\n\n\n```{r echo=F, message = F, warning = F}\nset.seed(5073)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(naniar)\nlibrary(janitor)\nlibrary(tidymodels)\nlibrary(rpart)\nlibrary(recipes)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(themis)\n\naval <- read.csv(\"scotland_avalanche_forecasts_2009_2025.csv\")\n```\n\n\n\n```{r echo=F}\n# quick checks\n#colnames(aval)\n#colSums(is.na(aval)) # number of missing values in each column\n#str(aval) # data type of each column\n\n# remove OAH: observed avalanche hazard\naval <- aval %>% select(-OAH)\n```\n\n# Data preparation: variable types\nWe begin by doing the following:\n1. Change the 'Date' variable to a Date class and extract the year, month and day of year (doy) as new variables.\n2. Create a new variable called 'Season' that groups the months together into seasons\n3. Ensure text variables are character classes and that indicators are factors\n4. Create a new variable for the response called FAH_ord, that casts FAH as a factor and assigns the different values to ordered levels\n5. Convert all the wind direction and aspect variables to their sine and cosine versions so that degrees that are far apart numerically but close together geographically would be close together numerically as well (ex. 0 degrees and 350 degrees).\n\n```{r echo=F}\n# clean the data by casting variables as correct types\n# STEP 1 — Fix types & engineer helper features (with OAH removed)\n\naval <- aval %>%\n  mutate(\n    # --- time features ---\n    DateTime = ymd_hms(Date, quiet = TRUE),\n    # strip out year, month and day:\n    Date     = as.Date(DateTime),\n    year     = year(Date),\n    month    = month(Date),\n    doy      = yday(Date),\n    # create a new variable: season\n    season   = factor(case_when(\n      month %in% c(12,1,2) ~ \"DJF\",\n      month %in% c(3,4,5)  ~ \"MAM\",\n      month %in% c(6,7,8)  ~ \"JJA\",\n      TRUE                 ~ \"SON\"\n    ), levels = c(\"DJF\",\"MAM\",\"JJA\",\"SON\")),\n\n    # --- categorical / IDs ---\n    Area     = factor(Area),\n    OSgrid   = as.character(OSgrid),\n    Location = as.character(Location),\n    Obs      = factor(Obs),\n    Drift = factor(Drift),\n    Rain.at.900 = factor(Rain.at.900),\n\n    # --- hazard (target) as ordered factor ---\n    FAH_ord  = factor(\n      FAH,\n      # specify the following order:\n      levels  = c(\"Low\",\"Moderate\",\"Considerable -\",\"Considerable +\",\"High\"),\n      ordered = TRUE\n    )\n  )\n```\n\nSome specific questions regarding the data:\n1. Is longitude and latitude constant within OSgrid?\n2. Is Alt constant within OSgrid?\n\n```{r echo=F}\n# is long and lat constant within OSgrid?\naval %>%\n  group_by(OSgrid) %>%\n  summarise(n_coords = n_distinct(paste(longitude, latitude)), .groups = \"drop\") %>%\n  filter(n_coords > 1)\n# yes, thus long and lat is the coordinates of different sites, OSgrid\n\n# is Alt the same within each OSgrid?\naval %>%\n  group_by(OSgrid) %>%\n  summarise(n_alt = n_distinct(Alt), .groups = \"drop\") %>%\n  filter(n_alt > 1)\n## no: cannot use Osgrid to impute missing Alt\n```\n\n# Investigate duplicates\n\n```{r}\n# should we add area, i.e. do multiple areas exist per (Date, OSgrid)\naval %>%\n  count(Date, OSgrid, Area) %>%\n  count(Date, OSgrid) %>%\n  filter(n > 1)  # there are some, so add area.\n\naval %>% \n  add_count(Date, OSgrid, Area, name=\"dup_n\") %>% \n  filter(dup_n > 1)\n```\n\n\nSome of these rows don't just differ by NA, but have conflicting numeric values. For these, keep both records, but for those that only differ by an NA, collapse the rows to keep the most numeric values per column.\n\n```{r}\n## 0) Define and validate keys -----------------------------------------------\nkey_cols <- c(\"Date\", \"OSgrid\", \"Area\")\nmissing_keys <- setdiff(key_cols, names(aval))\nif (length(missing_keys)) {\n  stop(\"These key columns are not in `aval`: \", paste(missing_keys, collapse = \", \"))\n}\n\n## 1) Build candidate non-key columns once (and be explicit) -----------------\nnon_key_cols <- setdiff(names(aval), key_cols)\n\n## 2) Find conflicts in ANY column type (numeric OR categorical) -------------\n# - use any_of() so it won't error if something is off\n# - n_distinct on na.omit for each column within the key\nwide_conflicts <- aval %>%\n  group_by(across(all_of(key_cols))) %>%\n  summarise(\n    across(all_of(non_key_cols), ~ n_distinct(na.omit(.x)), .names = \"nuniq_{.col}\"),\n    .groups = \"drop\"\n  ) %>%\n  # mark groups where any column has >1 distinct observed value\n  mutate(any_conflict = if_any(starts_with(\"nuniq_\"), ~ .x > 1)) %>%\n  filter(any_conflict) %>%\n  select(all_of(key_cols)) %>%\n  distinct()\n\n## 3) Collapse only conflict-free groups -------------------------------------\n# typed NA + first_non_na for safe collapsing\n\nfirst_non_na <- function(x) {\n  i <- which(!is.na(x))[1]\n  if (is.na(i)) x[NA_integer_] else x[i]\n}\n\ncollapsed_ok <- aval %>%\n  anti_join(wide_conflicts, by = key_cols) %>%\n  group_by(across(all_of(key_cols))) %>%\n  summarise(\n    across(all_of(non_key_cols), first_non_na),\n    .rows_collapsed = dplyr::n(),\n    .collapsed = TRUE,\n    .groups = \"drop\"\n  )\n\n## 4) Keep conflicting groups as-is (flag them) -------------------------------\nkept_conflicts <- aval %>%\n  semi_join(wide_conflicts, by = key_cols) %>%\n  mutate(.collapsed = FALSE)\n\n## 5) Combine and (optionally) drop flags ------------------------------------\naval_dups_resolved <- bind_rows(collapsed_ok, kept_conflicts) %>%\n  arrange(across(all_of(key_cols)))\n\naval <- aval_dups_resolved %>% select(-.collapsed, -.rows_collapsed)\n```\n\n\n# Investigate missing data\n\n## Visualise missingness\nDetermine the percentage missing values per variable and visualise this.\n\n```{r echo=F}\n# Per-variable % missing\nmiss_summary <- aval %>%\n  summarise(across(\n    everything(),\n    ~mean(is.na(.)) * 100\n  )) %>%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"pct_missing\") %>%\n  arrange(desc(pct_missing))\n\nmiss_summary\n\n# Visual overview\nnaniar::gg_miss_var(aval)\nnaniar::vis_miss(aval)\n```\n\n## Strategy for missing values\n\nFrom the above, only 10 variables have more than 5% of their values missing. Careful attention is paid to these below. The remaining variables have $<5\\%$ missing values and will simply be imputed with a Bagged-tree imputation approach.\n\nOf the variables missing more than 5%, we first need to determine if the missingness carries meaning. The list of variables is thus split in two, one where missingness does carry meaning and need to be accounted for and one where it does not.\n\nThe variables that do need to be accounted for are:\n- **`AV.Cat` (23.4%)**: Missing avalanche category values likely mean that no category was assigned for that day. Forecasters usually provide a category when avalanches are observed or when conditions are clear enough to classify. If it is missing, that could itself indicate that avalanches were not observed or that conditions were uncertain, which is meaningful information about overall stability.  \n\n- **`Ski.Pen` (22.5%)**: Ski penetration is only recorded when conditions allow observers to ski on the slope. If this field is missing, it often means the snow was too hard, too shallow, or otherwise unsuitable for skiing. This absence therefore reflects snow surface properties that can be related to avalanche hazard.  \n\n- **`Crystals` (9.3%)**: Crystal type is identified through snow-pit observations. Missing values here often mean that no pit was dug on that day, which in turn may depend on perceived stability, time constraints, or safety concerns. Thus, the lack of a crystal observation can itself provide information about conditions.  \n\n- **`Wetness` (5.4%)**: Wetness is typically noted when meltwater or damp snow is present. If this field is missing, it may indicate that the snow was dry and that observers did not see a reason to record wetness. Hence, missingness can indirectly point to dry-snow conditions.  \n\n- **`Snow.Index` (7.0%)**: This is a derived stability metric based on snowpack tests. If the value is missing, it likely means the relevant tests were not carried out, perhaps because conditions didn’t warrant them. This absence can therefore reflect judgments about snow stability.  \n\n- **`Summit.Wind.Dir_sin / Summit.Wind.Dir_cos` (12.4%), `Summit.Wind.Speed` (8.5%), and `Summit.Air.Temp` (7.1%)**: Missing summit weather variables may not just be sensor errors. It is plausible that readings were unavailable because weather at the summit was too extreme or dangerous for measurement, such as during storms or blizzards. In that case, missingness itself could be linked to hazardous conditions.  \n\nFor each of these, an indicator will be created to show if the value was missing.\n\nThose variables that do not need explicit missingness indicators are:\n\n- **`Max.Temp.Grad` (6.7%)**: This variable reflects temperature gradients measured in snow-pit tests. When missing, it is usually because the snow-pit test was not performed. However, the decision not to perform a pit is already captured by other variables where missingness is more clearly informative (e.g. `Crystals`, `Snow.Index`). Adding another indicator here would add redundancy without extra insight. The values themselves will be imputed with KNN.  \n\n- **`Max.Hardness.Grad` (5.9%)**: Like `Max.Temp.Grad`, hardness gradients are only measured in pits. Missing values again overlap with the same “pit not performed” scenario already captured by other indicators. For this reason, a separate indicator is unnecessary. The variable will be imputed with KNN to fill the missing numeric values.  \n\nAfter the relevant indicators are created, all remaining missing values will be imputed with **Bagged tree imputation**. Bagged tree imputation is a machine-learning approach where missing values in a variable are predicted using an ensemble of decision trees fit on the observed cases. Each tree is trained on a bootstrap sample of the data, and predictions are averaged across trees to produce stable and robust imputations. Unlike simple mean/median imputation or KNN, bagged trees can capture non-linear relationships and interactions among predictors, making them well suited to complex, structured data. \n\nIn the avalanche dataset, where variables combine topography, weather, and snowpack characteristics, and missingness can depend on multiple interacting factors, bagged tree imputation offers a principled way to exploit those dependencies while limiting noise from any single predictor. This allows us to fill gaps more realistically while preserving the multivariate structure that is important for downstream modeling with neural networks.\n\n\n```{r echo = F}\n# --- (A) Ensure target exists & indicators (0/1) are present ---\naval <- aval %>%\n  mutate(\n    av_cat_missing_initial            = as.integer(is.na(AV.Cat)),\n    ski_pen_missing_initial           = as.integer(is.na(Ski.Pen)),\n    crystals_missing_initial          = as.integer(is.na(Crystals)),\n    wetness_missing_initial           = as.integer(is.na(Wetness)),\n    snow_index_missing_initial        = as.integer(is.na(Snow.Index)),\n    summit_wind_dir_missing_initial   = as.integer(is.na(Summit.Wind.Dir)),\n    summit_wind_speed_missing_initial = as.integer(is.na(Summit.Wind.Speed)),\n    summit_air_temp_missing_initial   = as.integer(is.na(Summit.Air.Temp))\n  )\n\n```\n\nThe imputation will be done at a later stage. At first, the data still needs to be cleaned. Note that these missing values were intentionally set now before looking at improbable or outlier values below that are then encoded as NA. The reason for this is to truly only capture \"meaningful\" missingness in these indicators and not convolute them with missingness due to incorrect values being entered.\n\n# Cleaning of continuous variables\n\n```{r}\nnums <- names(dplyr::select(aval, where(is.numeric)))\nn_tot <- nrow(aval)\n\ncont_summary <- tibble(variable = nums) %>%\n  rowwise() %>%\n  mutate(\n    n_nonmiss   = sum(!is.na(aval[[variable]])),\n    n_missing   = n_tot - n_nonmiss,\n    pct_missing = round(100 * n_missing / n_tot, 2),\n    mean   = if (n_nonmiss > 0) mean(aval[[variable]], na.rm = TRUE)   else NA_real_,\n    median = if (n_nonmiss > 0) median(aval[[variable]], na.rm = TRUE) else NA_real_,\n    min    = if (n_nonmiss > 0) min(aval[[variable]], na.rm = TRUE)    else NA_real_,\n    max    = if (n_nonmiss > 0) max(aval[[variable]], na.rm = TRUE)    else NA_real_,\n    sd     = if (n_nonmiss > 1) sd(aval[[variable]],  na.rm = TRUE)    else NA_real_\n  ) %>%\n  ungroup() %>%\n  arrange(desc(pct_missing), variable)\n\ncont_summary\n```\n\n### Resonable variables\nThe following variables seemed reasonable in terms of outliers or impossible values:\n1. AV.Cat\n2. Crystals\n3. Summit.Air.Temp\n4. Max.Hardness.Grad\n5. Air Temperature\n6. Summit.Wind.Speed (assume wind speeds are in km/h)\n7. Wind.Speed (assume wind speeds are in km/h)\n\nThe remaining numerical variables were all cleaned.\n\n\n### Ski Penetration\n\n```{r}\nhist(aval$Ski.Pen)\nlength(which(aval$Ski.Pen<0))\n\n# remove all values <0:\naval$Ski.Pen[aval$Ski.Pen<0] <- NA_real_\n```\n\n### Snow Index\nSnow.Index is a derived stability score based on field observations such as penetration tests, hardness gradients, and crystal type. \n\nInitial suspicion: At first glance, the distribution of Snow.Index raised concerns because the vast majority of observations (over 7,700 of 10,671) had a value of zero, with only 57 unique values overall. Given the large number of missing entries in related snowpit variables, this pattern suggested that a zero might be functioning as a placeholder for “no measurement taken,” rather than a genuine stability score.\n\n```{r}\n# unique(aval$Snow.Index)\n# length(unique(aval$Snow.Index)) #57\nhist(aval$Snow.Index, breaks = 30)\n\n# length(which(aval$Snow.Index>100)) # 28\n# length(which(aval$Snow.Index>50)) # 35\n# length(which(aval$Snow.Index<0)) # 70\n# length(which(aval$Snow.Index>0 & aval$Snow.Index<20)) # 2021\n# length(which(aval$Snow.Index==0)) # 7770 actually equals 0\n```\n\nLet's try to confirm this suspicion by doing some checks. The following table looks at the distribution of values for other variables whre Snow.Index is zero.\n\n```{r}\n# Which rows have Snow.Index == 0 ?\nidx_zero <- aval$Snow.Index == 0\n\n# Compare % missing in other key variables for zero vs non-zero Snow.Index\naval %>%\n  mutate(group = case_when(\n    Snow.Index == 0 ~ \"Zero\",\n    is.na(Snow.Index) ~ \"NA\",\n    TRUE ~ \"Non-zero\"\n  )) %>%\n  summarise(\n    across(c(Ski.Pen, Foot.Pen, Crystals, Wetness, Max.Temp.Grad, Max.Hardness.Grad),\n           ~ mean(is.na(.)), \n           .names = \"{.col}_missing_rate\"),\n    n = n(),\n    .by = group\n  )\n```\n\nFurther inspection: However, when the missingness patterns were examined, rows with a Snow.Index of zero showed very low missingness in other snowpit variables (typically less than 10%). This indicates that these zeros are not simply standing in for absent measurements. Instead, they appear to represent true observations where no instability was detected. On this basis, zeros were retained as valid values, and only genuine missing entries (NA) were flagged for imputation.\n\n\n\n\n### Wetness\n\n```{r}\nhist(aval$Wetness)\nunique(aval$Wetness) # looks more like an index, keep as is.\n# length(which(aval$Wetness>9))\n```\n\n### Insolation\nInsolation measures the incoming solar radiation at the observation site, here recorded on a coded scale ranging mostly from 0–20. This index reflects sunlight exposure, which plays an important role in snowpack warming and wet-snow avalanche activity. A small number of observations fall outside the expected range, including negative values and extreme outliers above 20 (e.g., -55, 106, 208). Since these are not physically meaningful, they were treated as data entry errors and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Insolation)\nunique(aval$Insolation)\ntable(aval$Insolation)\n\n# encode <0 and >20 as NA\naval$Insolation[which(aval$Insolation<0 |\n                        aval$Insolation>20)] <- NA_real_\n```\n\n### Snow Temperature\n\nSnow.Temp records the snowpack temperature in degrees Celsius. Values in this dataset range from -13 to 124 °C. Since snow cannot persist above 0 °C, values above 5 °C were considered physically implausible and were recoded as missing for later imputation. Negative values down to -13 °C were retained as realistic cold-snow conditions.\n\n```{r}\nhist(aval$Snow.Temp)\nsort(unique(aval$Snow.Temp))\nlength(which(aval$Snow.Temp>5)) # 176\nlength(which(aval$Snow.Temp<10)) # 10127\n\n# make all values above 5 NA\naval$Snow.Temp[which(aval$Snow.Temp>5)] <- NA_real_\n```\n\n### Aspect\n\nAspect describes the compass direction of the slope, measured in degrees clockwise from north (0–360°). It influences how much solar radiation a slope receives and therefore affects snow metamorphism and avalanche risk. In the dataset, any values outside the valid range of 0–360° were considered invalid and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Aspect)\nlength(which(aval$Aspect>360)) # only 8- make these NA\nlength(which(aval$Aspect<0)) # only 4- make these NA\n\n# make values >360 and <0 NA:\naval$Aspect[which(aval$Aspect<0 |\n                    aval$Aspect>360)] <- NA_real_\n```\n\n### No.Settle\n\n```{r}\nhist(aval$No.Settle) # looks fine\n```\n\n\n\n\n### Total Snow Depth\nTotal Snow depth represents the measured thickness of the snowpack in centimeters. Most values lie between 0–500 cm, which is consistent with expected conditions in the observation areas. Negative values are not physically possible, while a small number of extreme outliers above 500 cm were judged unrealistic for the region. These implausible entries were recoded as missing for later imputation.\n\n```{r}\nhist(aval$Total.Snow.Depth)\nlength(which(aval$Total.Snow.Depth>500)) # only 14\nlength(which(aval$Total.Snow.Depth<0)) # only 38\n\naval$Total.Snow.Depth[which(aval$Total.Snow.Depth>500|\n                              aval$Total.Snow.Depth<0)] <- NA_real_\n```\n\n\n### Wind Direction and Summit Wind Direction\nWind direction values less than 0° or greater than 360° are not physically possible. Since the source of these errors is unknown, they were treated as missing (NA) rather than corrected by assumption. These missing values will later be imputed using bagged-tree models, with a corresponding missingness indicator retained to capture any systematic patterns in measurement failure.\n\n```{r}\nhist(aval$Summit.Wind.Dir) # clear outliers/ noise\nhist(aval$Wind.Dir) # clear outliers/ noise\n# length(which(aval$Summit.Wind.Dir>360)) # 4\n# length(which(aval$Summit.Wind.Dir<0)) # 191\n# length(which(aval$Wind.Dir>360)) # 1\n# length(which(aval$Wind.Dir<0)) # 13\n\n# assign NA to these and the cos and sin versions:\n# 1) Identify invalid entries (out of [0, 360])\nidx_wind_invalid   <- !is.na(aval$Wind.Dir)        & (aval$Wind.Dir < 0 | aval$Wind.Dir > 360)\nidx_summit_invalid <- !is.na(aval$Summit.Wind.Dir) & (aval$Summit.Wind.Dir < 0 | aval$Summit.Wind.Dir > 360)\n# sum(idx_wind_invalid) # 14\n# sum(idx_summit_invalid) # 195\n\n# 2) Set invalid directions to NA\naval$Wind.Dir[idx_wind_invalid]               <- NA_real_\naval$Summit.Wind.Dir[idx_summit_invalid]      <- NA_real_\n```\n\n\n### Foot Penetration\nFoot penetration measures the depth in centimeters that an observer’s boot sinks into the snow surface. It reflects snow hardness and helps assess surface stability. Typical values range from a few centimeters in firm snow to over a meter in very soft conditions. In the dataset, negative values and extreme entries above 100 cm were judged implausible and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Foot.Pen)\nlength(which(aval$Foot.Pen>100)) # only 3\nlength(which(aval$Foot.Pen<0)) # only 4\n\naval$Foot.Pen[which(aval$Foot.Pen<0|\n                      aval$Foot.Pen>100)] <- NA_real_\n```\n\n### Incline\nIncline represents the slope angle at the observation site, recorded in degrees. Since slope angle is central to avalanche release, accurate measurement is critical. Values should range from 0° (flat) to 90° (vertical). In the dataset, negative values and outliers above 90° were considered invalid and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Incline)\nlength(which(aval$Incline>90)) # only 4\nlength(which(aval$Incline<0)) # only 3\n\naval$Incline[which(aval$Incline<0|\n                     aval$Incline>90)] <- NA_real_\n```\n\n### Cloud\n\nCloud represents the observed cloud cover, recorded as a percentage from 0 (clear sky) to 100 (fully overcast). This variable provides important context for snowpack conditions, since cloud cover influences surface cooling, radiation balance, and melting. In the dataset, most values fell within the expected 0–100% range. A small number of impossible values (e.g., -1) were considered data entry errors and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Cloud)\nlength(which(aval$Cloud > 100)) # only 1\nlength(which(aval$Cloud < 0))   # only 3\n\naval$Cloud[which(aval$Cloud < 0 |\n                   aval$Cloud > 100)] <- NA_real_\n```\n\n\n### Maximum temperature grading\nIn avalanche datasets, Max.Temp.Grad = Maximum Temperature Gradient within the snowpack profile.When snow profiles are dug, forecasters often measure snow temperature at different depths (surface, mid-pack, base). They then compute the temperature gradient (°C per 10 cm) between layers. The maximum gradient across all measured layers is recorded as Max.Temp.Grad.\n\nIn general, we would expect values between 0-5 to be very common, 5-10 to be less common, but plausible and values of greater than 10 to be very rare.  Any value greater than 10 was assumed to be an error and was replaced with an NA value.\n\n```{r}\nhist(aval$Max.Temp.Grad) # clear outliers/ noise, a value of 130 is physically impossible\nlength(which(aval$Max.Temp.Grad>10)) # 160 \n\naval$Max.Temp.Grad[which(aval$Max.Temp.Grad>10)] <- NA_real_\n```\n\n\n### Altitude\n\nAltitude values greater than 1400 m or less than 0 m are physically impossible within the Scottish study area (highest peak 1345 m). These were treated as missing for later imputation.\n\n```{r}\nhist(aval$Alt[aval$Alt<2000])\n# length(which(aval$Alt>10000)) # only 2\n\naval$Alt[aval$Alt < 0 | aval$Alt > 1400] <- NA_real_\n```\n\n\n```{r}\n# sum(is.na(aval$Alt)) # now 9 are missing\n```\n\nAfter replacing impossible values with NA, 9 observations had missing Altitude. Longitude and latitude were found to be constant within each OSgrid, i.e. each OSgrid corresponds to a unique coordinate pair. However, altitude was not unique within these coordinates, suggesting that the OSgrid represents an area grid of unknown size, while the longitude and latitude are specific sampling points within that grid.\n\nTo impute missing Altitude values, an open-source elevation API will be queried at the recorded longitude and latitude of each observation. This approach assumes that altitude variation within the grid is limited, and that the elevation at the given coordinates is representative for the corresponding observation. \n\nWhen doing this, it was noticed that one of the altitudes was zero. Upon further investigation, the point was found to be in Loch Fyne (Sea Loch), and was thus assumed to be an erroroneous longitude and latitude.\n\n```{r}\n# --- 1) Helper: call Open-Elevation for a batch of lat/lon points ---\n# Expects a tibble/data.frame with columns: latitude, longitude\n# Returns: tibble(latitude, longitude, elev_m)\noe_lookup_batch <- function(points_df) {\n  if (nrow(points_df) == 0) return(tibble(latitude = numeric(), longitude = numeric(), elev_m = numeric()))\n  # Build \"lat,lon|lat,lon|...\" string\n  locs <- points_df %>%\n    transmute(pair = sprintf(\"%.6f,%.6f\", latitude, longitude)) %>%\n    pull(pair) %>%\n    paste(collapse = \"|\")\n\n  url <- paste0(\"https://api.open-elevation.com/api/v1/lookup?locations=\", URLencode(locs))\n\n  resp <- request(url) |> req_timeout(30) |> req_perform()\n\n  if (resp_status(resp) != 200) {\n    warning(\"Open-Elevation request failed with status: \", resp_status(resp))\n    return(tibble(latitude = numeric(), longitude = numeric(), elev_m = numeric()))\n  }\n\n  dat <- resp_body_json(resp, simplifyVector = TRUE)\n  res <- as_tibble(dat$results)\n  # API returns 'elevation' (meters), 'latitude', 'longitude'\n  res %>%\n    transmute(\n      latitude  = as.numeric(latitude),\n      longitude = as.numeric(longitude),\n      elev_m    = as.numeric(elevation)\n    )\n}\n\n# --- 2) Wrapper: batch over many points, with simple rate limiting & safety ---\noe_lookup <- function(points_df, batch_size = 80, sleep_secs = 1) {\n  # round coords to reduce accidental duplicates/float precision issues\n  pts <- points_df %>%\n    transmute(\n      latitude  = round(as.numeric(latitude), 6),\n      longitude = round(as.numeric(longitude), 6)\n    ) %>%\n    distinct()\n\n  batches <- split(pts, ceiling(seq_len(nrow(pts)) / batch_size))\n\n  safe_batch <- safely(oe_lookup_batch, otherwise = tibble(latitude=numeric(), longitude=numeric(), elev_m=numeric()))\n  results <- map(batches, function(b) {\n    out <- safe_batch(b)\n    Sys.sleep(sleep_secs)\n    if (!is.null(out$error)) warning(\"Batch failed: \", out$error)\n    out$result\n  })\n\n  bind_rows(results)\n}\n\n# --- 3) Apply to your data: only rows with missing Alt ---\n# Assumes your data frame is named `aval` and has columns Alt, latitude, longitude\nto_fill <- aval %>%\n  filter(is.na(Alt)) %>%\n  transmute(latitude = latitude, longitude = longitude)\n\nelev_tbl <- oe_lookup(to_fill, batch_size = 80, sleep_secs = 1)\n\n# --- 4) Join back and fill Alt where missing ---\n# 1) Build a lookup key on the API results (rounded to match request precision)\nelev_lu <- elev_tbl %>%\n  mutate(key = paste0(round(latitude, 6), \"_\", round(longitude, 6))) %>%\n  select(key, elev_m)\n# one of the values is zero altitude: this is in the ocean (see map)\n\n# Coordinates whose elevation came back as exactly 0\nzero_pts <- elev_tbl %>%\n  filter(!is.na(elev_m), elev_m == 0) %>%\n  transmute(x = round(longitude, 6), y = round(latitude, 6)) %>%\n  distinct()\n```\n\nReplace this longitude and latitude with the average longitude and latitude in the area.\n\n```{r}\n# fixing the incorrect long and lat value\naval %>%\n  filter(round(longitude, 6) == zero_pts$x,\n         round(latitude, 6) == zero_pts$y)\n\n# it's the only observation in this grid\naval %>% \n  filter(OSgrid == \"NN077044\")\n\n# average long and lat in the area:\nmean_coord <- aval %>%\n  filter(Area == \"Creag Meagaidh\") %>%\n  summarise(\n    mean_lon = mean(longitude, na.rm = TRUE),\n    mean_lat = mean(latitude, na.rm = TRUE)\n  )\n\n# known bad coordinates (round to avoid float mismatch)\nbad_lon <- round(as.numeric(zero_pts$x), 6)\nbad_lat <- round(as.numeric(zero_pts$y), 6)\n\n# replacement mean values\nmean_lon <- mean_coord$mean_lon\nmean_lat <- mean_coord$mean_lat\n\n# replace in aval\naval <- aval %>%\n  mutate(\n    longitude = if_else(round(longitude, 6) == bad_lon & round(latitude, 6) == bad_lat,\n                        mean_lon, longitude),\n    latitude  = if_else(round(longitude, 6) == bad_lon & round(latitude, 6) == bad_lat,\n                        mean_lat, latitude)\n  )\n```\n\n\n```{r}\nto_fill <- aval %>%\n  filter(is.na(Alt)) %>%\n  transmute(latitude = latitude, longitude = longitude)\n\nelev_tbl <- oe_lookup(to_fill, batch_size = 80, sleep_secs = 1)\n\n# --- 4) Join back and fill Alt where missing ---\n# 1) Build a lookup key on the API results (rounded to match request precision)\nelev_lu <- elev_tbl %>%\n  mutate(key = paste0(round(latitude, 6), \"_\", round(longitude, 6))) %>%\n  select(key, elev_m)\n\n# 2) Build the same key for your aval rows\naval_key <- paste0(round(aval$latitude, 6), \"_\", round(aval$longitude, 6))\n\n# 3) Match each aval row to the API elevation\nmatch_idx <- match(aval_key, elev_lu$key)\nalt_from_api <- elev_lu$elev_m[match_idx]   # will be NA where no API result\n\n# 4) Replace Alt in place ONLY where it's missing and we have a lookup value\nfill_idx <- is.na(aval$Alt) & !is.na(alt_from_api)\naval$Alt[fill_idx] <- alt_from_api[fill_idx]\n\n# 5) Quick check\ncat(\"Alt missing before fill:\", sum(is.na(aval$Alt)) + sum(fill_idx == TRUE), \"\\n\")\ncat(\"Alt missing after fill :\", sum(is.na(aval$Alt)), \"\\n\")\n```\n\n# Encode angles as circular\n\n```{r}\n# After finishing all angle cleaning:\naval <- aval %>%\n  mutate(\n    Wind.Dir_sin        = if_else(is.na(Wind.Dir), NA_real_, sin(pi * Wind.Dir/180)),\n    Wind.Dir_cos        = if_else(is.na(Wind.Dir), NA_real_, cos(pi * Wind.Dir/180)),\n    Summit.Wind.Dir_sin = if_else(is.na(Summit.Wind.Dir), NA_real_, sin(pi * Summit.Wind.Dir/180)),\n    Summit.Wind.Dir_cos = if_else(is.na(Summit.Wind.Dir), NA_real_, cos(pi * Summit.Wind.Dir/180)),\n    Aspect_sin          = if_else(is.na(Aspect), NA_real_, sin(pi * Aspect/180)),\n    Aspect_cos          = if_else(is.na(Aspect), NA_real_, cos(pi * Aspect/180))\n  )\n```\n\n# Train- Test- Val split\nNote that imputation will be done after this as we will be using bagged trees and want to avoid data leakage. Also note that the split is not random, since this is time series data. Thus the first 70% of the data is the training set.\n\n```{r}\n# STEP 1 — Target + time-based splits ---------------------------------------\nset.seed(7)\n\n# 0) Ensure the columns we need exist & are the right type\nstopifnot(all(c(\"Date\") %in% names(aval)))\n\n# If FAH_ord doesn't exist or isn't correctly ordered, (re)create it from FAH\ntarget_levels <- c(\"Low\",\"Moderate\",\"Considerable -\",\"Considerable +\",\"High\")\nif (!(\"FAH_ord\" %in% names(aval)) ||\n    !is.ordered(aval$FAH_ord) ||\n    !identical(levels(aval$FAH_ord), target_levels)) {\n  stopifnot(\"FAH\" %in% names(aval))\n  aval <- aval %>%\n    mutate(FAH_ord = factor(FAH, levels = target_levels, ordered = TRUE))\n}\n\n# Parse Date robustly if needed\nif (!inherits(aval$Date, \"Date\")) {\n  aval <- aval %>%\n    mutate(Date = as.Date(parse_date_time(\n      Date, orders = c(\"ymd HMS\",\"ymd HM\",\"ymd\",\"dmy HMS\",\"dmy HM\",\"dmy\")\n    )))\n}\n\n# Drop rows with missing target or Date (NNs can't train on NA targets)\naval <- aval %>% filter(!is.na(Date), !is.na(FAH_ord))\n\n# 1) Create time-based splits: 70% train, 15% val, 15% test by calendar time\ncut1 <- quantile(aval$Date, probs = 0.80, na.rm = TRUE, type = 1)\n#cut2 <- quantile(aval$Date, probs = 0.85, na.rm = TRUE, type = 1)\n\ntrain <- aval %>% filter(Date <= cut1)\n#val   <- aval %>% filter(Date >  cut1 & Date <= cut2)\ntest  <- aval %>% filter(Date >  cut1)#cut2)\n\n# Sanity checks: no overlap & correct ordering\n#stopifnot(max(train$Date) < min(val$Date), max(val$Date) < min(test$Date))\n\n# 2) Quick class balance check (important for NN)\ncat(\"Train date range: \", min(train$Date), \"to\", max(train$Date), \"\\n\")\n#cat(\"Val   date range: \", min(val$Date),   \"to\", max(val$Date),   \"\\n\")\ncat(\"Test  date range: \", min(test$Date),  \"to\", max(test$Date),  \"\\n\\n\")\n\ncat(\"Class counts (train):\\n\"); print(table(train$FAH_ord))\ncat(\"\\nClass proportions (train):\\n\"); print(round(prop.table(table(train$FAH_ord)), 3))\n\n#cat(\"\\nClass counts (val):\\n\"); print(table(val$FAH_ord))\ncat(\"\\nClass counts (test):\\n\"); print(table(test$FAH_ord))\n```\nNote the imbalance in FAH_ord. The test set has no High cases. But, the training set has enough data to train on to be able to identify High risk cases.\n\n```{r}\n# extract dates \n\ntrain_dates = train$Date\ntest_dates = test$Date\n```\n\n\n# Pre-processing data\n\n```{r}\n# STEP 2 — NN-ready preprocessing with recipes --------------------------------\n\n# 1) Choose columns to DROP (IDs, raw angles, dates, free text) ---------------\ndrop_cols <- intersect(\n  c(\n    \"FAH\",                # raw target (we'll use FAH_ord)\n    \"Date\", \"DateTime\",   # time stamps (we keep year/month/doy/season)\n    \"Wind.Dir\", \"Summit.Wind.Dir\", \"Aspect\",   # raw angles (keep sin/cos)\n    \"OSgrid\",             # grid ID (too high-cardinality for one-hot)\n    \"Location\"            # often messy/free-text; drop for NN baseline\n  ),\n  names(train)\n)\n\n# 2) Identify informative-missing flags so we can avoid scaling them ----------\nflag_prefixes <- c(\n  \"av_cat_missing\", \"ski_pen_missing\", \"crystals_missing\",\n  \"wetness_missing\", \"snow_index_missing\",\n  \"summit_wind_dir_missing\", \"summit_wind_speed_missing\", \"summit_air_temp_missing\"\n)\nflag_cols <- unique(unlist(lapply(flag_prefixes, function(p) grep(p, names(train), value = TRUE))))\n# You may have named them with `_initial`. If so, they’ll be picked up by grep above.\n\n#flag_cols = c(flag_cols, \"Area\")\n\n# 3) Build the recipe ---------------------------------------------------------\nrec <- recipe(FAH_ord ~ ., data = train) %>% \n  # drop columns we don't want to feed to the NN\n  step_rm(all_of(drop_cols)) %>%\n  # explicitly confirm Area is a predictor (this line is optional, since it’s already a predictor by default)\n  update_role(Area, new_role = \"predictor\") %>%\n  # collapse *very* rare categories to \"other\"\n  step_other(all_nominal_predictors(), threshold = 0.005, other = \"other\") %>% \n  # impute categoricals by mode\n  step_impute_mode(all_nominal_predictors()) %>%\n  # impute numerics by bagged trees\n  step_impute_bag(all_numeric_predictors()) %>%\n  # one-hot encode categoricals\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%\n  # drop zero-variance and near-zero-variance columns\n  step_zv(all_predictors()) %>%\n  step_nzv(all_predictors()) %>%\n  # scale/center numerics (excluding the missingness flags)\n  step_normalize(all_numeric_predictors(), -all_of(c(\"Area_Creag.Meagaidh\",  \"Area_Glencoe\",             \"Area_Lochaber\",\"Area_Northern.Cairngorms\", \"Area_Southern.Cairngorms\" ,\"Area_Torridon\",flag_cols)))\n\n# 4) Prep on TRAIN only, then bake to splits ---------------------------------\nrec_prep <- prep(rec, training = train, retain = TRUE)\n# this calculates the mean/sd for scaling, finds the mode of categoricals, trains the bagged trees for imputation.\n\nx_train <- bake(rec_prep, new_data = train) # apply to train set\n#x_val   <- bake(rec_prep, new_data = val) # apply to val set\nx_test  <- bake(rec_prep, new_data = test) # apply to test set\n\n# 5) Quick sanity checks ------------------------------------------------------\ncat(\"Shapes:\\n\")\ncat(\"  train:\", dim(x_train)[1], \"rows x\", dim(x_train)[2], \"cols\\n\")\n#cat(\"  val  :\", dim(x_val)[1],   \"rows x\", dim(x_val)[2],   \"cols\\n\")\ncat(\"  test :\", dim(x_test)[1],  \"rows x\", dim(x_test)[2],  \"cols\\n\\n\")\n\ncat(\"Any NA left? \",\n    any(vapply(x_train, function(col) any(is.na(col)), logical(1))),\n   # any(vapply(x_val,   function(col) any(is.na(col)), logical(1))),\n    any(vapply(x_test,  function(col) any(is.na(col)), logical(1))), \"\\n\")\n\ncat(\"\\nTarget distribution after preprocessing (train):\\n\")\nprint(table(x_train$FAH_ord))\n\ncat(\"\\nTarget distribution after preprocessing (test):\\n\")\nprint(table(x_test$FAH_ord))\n```\nchange splits to 0.8 train 0.2 test\n\nTalk to data set balance \n\n\n## Changes \n\n- remove unnecessary columns \n- encoded categorical and binary variables \n\n\n```{r}\n\n#encode reponse variable as integers, presevre ordinal property\n\nlev = c(\"Low\", \"Moderate\", \"Considerable-\", \"Considerable+\", \"High\")\n\nX_train = x_train %>% mutate(\n  #FAH_ord = factor(FAH, )\n  FAH_ord= as.integer(FAH_ord) - 1\n  )\n\n# X_val = x_val %>% mutate(\n#   #FAH_ord = factor(FAH, )\n#   FAH_ord= as.integer(FAH_ord) - 1\n#   )\n\nX_test = x_test %>% mutate(\n  #FAH_ord = factor(FAH, )\n  FAH_ord= as.integer(FAH_ord) - 1\n  )\n```\n\n\nremove month and doy column, rebind full dates for sequence generation in python (keep year), as well as missingness cols \n\n```{r}\n\n#remove doy and month , and NOT missingness indicators\n\nX_train = X_train[, -(22:24)]\nX_test = X_test[, -(22:24)]\n\n#rebind dates \n\nX_train = cbind(train_dates, X_train)\nX_test = cbind(test_dates, X_test)\n\n```\n\n\nReplace the season binaries with a doy sin/cos for smooth periodicity\n\n```{r}\nX_train <- X_train %>%\n  mutate(\n    doy = yday(train_dates),                              # 1..365/366\n    period = if_else(leap_year(train_dates), 366, 365),   # handle leap years\n    doy_sin = sin(2*pi * doy / period),\n    doy_cos = cos(2*pi * doy / period)\n  ) %>%\n  select(-doy, -period)  # optional: drop helpers\n\n#remove month binaries \n\nX_train = X_train %>% \n  select(-season_DJF, -season_MAM)\n\nX_test <- X_test %>%\n  mutate(\n    doy = yday(test_dates),                              # 1..365/366\n    period = if_else(leap_year(test_dates), 366, 365),   # handle leap years\n    doy_sin = sin(2*pi * doy / period),\n    doy_cos = cos(2*pi * doy / period)\n  ) %>%\n  select(-doy, -period)  # optional: drop helpers\n\n#remove month binaries \n\nX_test = X_test %>% \n  select(-season_DJF, -season_MAM)\n\n```\n\n\nwrite to csv\n\n```{r}\nwrite.csv(X_train, \"X_train.csv\")\nwrite.csv(X_test, \"X_test.csv\")\n```\n\nwill isolate response and do validation windown splits in python\n\nremove initial missing windows\n\n```{r}\n\nX_train = read.csv(\"X_train.csv\")\n\n```\n\n\n```{python}\nimport random\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader, Subset\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\nimport torch.nn.functional as F\nimport numpy as np\n\n#from google.colab import files\nimport pandas as pd\nimport io\n\nimport pandas as pd # Example library for loading data\n\n#replace with your file path\nfile_path = \"C:/Users/chris/OneDrive/Documents/UNI/STATS/Masters/DS4I/Assignment/Avalanche-Forecast-Prediction/X_train.csv\"\n\n# 2. Check if the path is provided\nif file_path:\n    print(f\"Attempting to load: {file_path}\")\n    \n    # 3. Load the file\n    try:\n        # Note: If the user pastes a Windows path with single backslashes (\\), \n        # Python might need double backslashes (\\\\) or forward slashes (/) to work correctly.\n        # It's safest to manually replace them or ask the user to use forward slashes.\n        train = pd.read_csv(file_path.replace('\\\\', '/')) \n        print(\"File loaded successfully!\")\n        print(train.head())\n    except FileNotFoundError:\n        print(\"Error: The file path was not found.\")\n    except Exception as e:\n        print(f\"Error loading file: {e}\")\nelse:\n    print(\"No path provided.\")\n```\n\n```{python}\n#replace with your file path\nfile_path = \"C:/Users/chris/OneDrive/Documents/UNI/STATS/Masters/DS4I/Assignment/Avalanche-Forecast-Prediction/X_test.csv\"\n\n# 2. Check if the path is provided\nif file_path:\n    print(f\"Attempting to load: {file_path}\")\n    \n    # 3. Load the file\n    try:\n        # Note: If the user pastes a Windows path with single backslashes (\\), \n        # Python might need double backslashes (\\\\) or forward slashes (/) to work correctly.\n        # It's safest to manually replace them or ask the user to use forward slashes.\n        test = pd.read_csv(file_path.replace('\\\\', '/')) \n        print(\"File loaded successfully!\")\n        print(test.head())\n    except FileNotFoundError:\n        print(\"Error: The file path was not found.\")\n    except Exception as e:\n        print(f\"Error loading file: {e}\")\nelse:\n    print(\"No path provided.\")\n```\n\nPrepare Sequences \n\n```{python}\ntarget_col = \"FAH_ord\"\n\ntrain.rename(columns={train.columns[1]: 'Date'}, inplace=True)\ntest.rename(columns={test.columns[1]: 'Date'}, inplace=True)\n\n#remove carried over index column\ntrain.drop(columns=train.columns[0], inplace=True)\ntest.drop(columns=test.columns[0], inplace=True)\n\ndate_col   = \"Date\"\narea_cols  = [c for c in train.columns if c.startswith(\"Area_\")]     # one-hot 0/1\nstatic_num = [c for c in [\"longitude\",\"latitude\",\"Alt\",\"Incline\"] if c in train.columns]\nstatic_cols = static_num + area_cols                                 # fed once per window\n\n# dynamic = numeric columns minus target, static, and labels\ndef numeric_cols(df): return df.select_dtypes(include=[np.number]).columns.tolist()\nban = set([target_col]) | set(static_cols)\ndynamic_cols = [c for c in numeric_cols(train) if c not in ban]\n\ntrain[\"_src\"] = \"train\"; test[\"_src\"] = \"test\"\ndf = pd.concat([train, test], ignore_index=True)\ntest_start_ts = pd.to_datetime(test[date_col]).min()   # Timestamp\nassert pd.notna(test_start_ts), \"Could not determine test start date.\"\n\n# Area id from one-hots\ndf[\"__area_id__\"] = np.argmax(df[area_cols].values, axis=1)\ndf = df.sort_values([\"__area_id__\", date_col]).reset_index(drop=True)\n```\n\nadd in previous forecast to predict next forecast\n\n```{python}\n\n#before building windows:\ndf = df.sort_values([\"__area_id__\", date_col])\ndf[\"FAH_prev\"] = (df.groupby(\"__area_id__\")[target_col]\n                    .shift(1).astype(\"float32\"))\ndf[\"FAH_prev\"] = df[\"FAH_prev\"].fillna(0)   # or area-wise mean\n\n# add to dynamic cols if not there already\nif \"FAH_prev\" not in dynamic_cols:\n    dynamic_cols.append(\"FAH_prev\")\n\n# rebuild windows (same L,H) to get new Xseq_* with the extra column\n\n\n```\n\n```{python}\n\ntrain.head()\ntest.head()\n\n```\n\n```{python}\n\n#before building windows:\ndf = df.sort_values([\"__area_id__\", date_col])\ndf[\"FAH_prev\"] = (df.groupby(\"__area_id__\")[target_col]\n                    .shift(1).astype(\"float32\"))\ndf[\"FAH_prev\"] = df[\"FAH_prev\"].fillna(0)   # or area-wise mean\n\n# add to dynamic cols if not there already\nif \"FAH_prev\" not in dynamic_cols:\n    dynamic_cols.append(\"FAH_prev\")\n\n# rebuild windows (same L,H) to get new Xseq_* with the extra column\n\n```\n\nMakes the sequences for the LSTM, of L length (usually 7, 14)\n\n```{python}\nL, H = 14, 0  # look-back and horizon\ndef build_windows_with_dates(df, L, H, dyn_cols, sta_cols, target_col, date_col):\n    X_seq, X_sta, y, tgt_dates = [], [], [], []\n    for a, g in df.groupby(\"__area_id__\", sort=False):\n        g = g.sort_values(date_col)\n        dyn = g[dyn_cols].to_numpy(np.float32)\n        sta = g[sta_cols].to_numpy(np.float32)\n        yy  = g[target_col].to_numpy(np.int64)\n        dd  = g[date_col].to_numpy('datetime64[ns]')\n        for t in range(L+H, len(g)):\n            X_seq.append(dyn[t-L-H:t-H])\n            X_sta.append(sta[t])\n            y.append(yy[t])\n            tgt_dates.append(dd[t])       # date of the target row\n    return (torch.tensor(np.stack(X_seq)),\n            torch.tensor(np.stack(X_sta)),\n            torch.tensor(np.array(y)),\n            np.array(tgt_dates))\n\nXseq_all, Xsta_all, y_all, tgt_dates = build_windows_with_dates(\n    df, L, H, dynamic_cols, static_cols, target_col, date_col\n)\n\n# 6) SAFE comparison: convert the Timestamp to numpy datetime64 *with unit*\ntest_start_np = np.datetime64(test_start_ts, 'ns')\nis_test  = tgt_dates >= test_start_np\nis_train = ~is_test\n\nXseq_tr, Xsta_tr, y_tr = Xseq_all[is_train], Xsta_all[is_train], y_all[is_train]\nXseq_te, Xsta_te, y_te = Xseq_all[is_test],  Xsta_all[is_test],  y_all[is_test]\n\nprint(\"Train:\", Xseq_tr.shape, Xsta_tr.shape, y_tr.shape)\nprint(\"Test :\", Xseq_te.shape, Xsta_te.shape, y_te.shape)\n```\n\n\nShows the dimensions of the test and train sequences. For train, 8365 sequences, of 14 observations each, of 44 variables. Slightly fewer sequences than there were observations in the train split, as lose some at the bounds where can't build full sequence. \n\n\nExample sequence\n```{python}\nprint(Xseq_tr[[1]])\n```\n\nmake sure no sequences target dates in test window\n\n```{python}\n# You already computed this earlier:\n# Xseq_all, Xsta_all, y_all, tgt_dates = build_windows_with_dates(...)\n\n# And the test boundary:\ntest_start_ts = pd.to_datetime(test[\"Date\"]).min()\ntest_start_np = np.datetime64(test_start_ts, 'ns')\n\n# Mask by each window's TARGET date\nis_test  = tgt_dates >= test_start_np\nis_train = ~is_test\n\n# Training/Testing target dates aligned to y_tr / y_te\ntgt_dates_tr = tgt_dates[is_train]\ntgt_dates_te = tgt_dates[is_test]\n\n# Sanity check\nassert len(tgt_dates_tr) == y_tr.shape[0]\nassert len(tgt_dates_te) == y_te.shape[0]\n```\n\nBreak sequences into folds that follow each other (forward chaining) for time aware validation split\n\neach fold strictly goes past to future, and never randomly shuffles\n\nso for fold k, validation is a contiguous block of unique target dates, and training is alll earlier windows except embargo of L (sequence length) days before validation block, which prevents feature leakage \n\nIs an expanding window so as move forward in time , train includes more history \n\n```{python}\n# ---------- 1) Build forward-chaining folds ----------\ndef make_time_folds(dates, K=5, gap_days=0):\n    \"\"\"Return list of (train_idx, val_idx) respecting time and an optional gap.\"\"\"\n    dates = np.array(dates)\n    uniq = np.unique(dates)\n    # split unique dates into K chronological bins (roughly equal by time span)\n    cuts = np.quantile(np.arange(len(uniq)), np.linspace(0,1,K+1)).round().astype(int)\n    folds = []\n    for k in range(K):\n        start_u, end_u = cuts[k], cuts[k+1]\n        val_days = set(uniq[start_u:end_u])\n        if not val_days:\n            continue\n        val_mask = np.array([d in val_days for d in dates])\n        # embargo/gap: drop train samples within gap_days before the val start\n        if gap_days > 0:\n            val_start = uniq[start_u]\n            gap_mask = dates >= (np.datetime64(val_start) - np.timedelta64(gap_days, 'D'))\n        else:\n            gap_mask = np.zeros_like(val_mask, dtype=bool)\n        train_mask = (~val_mask) & (~gap_mask) & (dates < np.max(list(val_days)))\n        tr_idx = np.where(train_mask)[0]\n        va_idx = np.where(val_mask)[0]\n        if len(tr_idx) and len(va_idx):\n            folds.append((tr_idx, va_idx))\n    return folds\n\n```\n\n## Neural Network architecture\n\nUsing two measures primarly: QWK and CORN\n\nQuadratic Weighted Kappa (QWK) measures agreement between two sets of ordinal ratings (predicted vs forecasted hazard level). Applies quadratic weighting - mistakes that are further apart are more penalised than close ones. lies on -1 to 1\n\nQWK is non-differntiable (can't be used in backprop), so not used in training only as metric\n\nInstead train with differentiable loss - CORN. QWK is used for validation and for early stopping\n\nCumulative Ordinal Regression for NN (CORN). \n\"CORN reframes the multi-class problem into a series of simpler, ordered binary classification tasks.\"\n\nChose CORN over CE because CE treats classes as nominal, so does not distinguish between misclassifications (treats all errors equally bad which is not ideal for ordinal case).\n\nCORN creates K-1 logits for binary question Pr(y>K), as opposed to CE which generates K softmax logits\n\n(note that the loss is on logits vs targets , not using thresholds. after training the logits are concerted to probabilities , which are then put against the thresholds for the binary questions.)\n\nIn the code , pr(y>k) is upweighted for rare hazards - combats imbalance (also have WeightedRandomSampler)\n\nFunctions:\n\ncorn_targets : builds K-1 binary labels for CORN. for each threshold k, target is 1 if y>k else 0\n\ncorn_loss : loss function used in training\n\ncorn_predict_logits_to_labels : converts logits to class predictions \n\n```{python}\nimport torch, torch.nn as nn, torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, Subset, WeightedRandomSampler\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nK = int(y_tr.max().item()) + 1  # e.g., 5 classes (0..4)\n\ndef corn_targets(y, K):\n    ks = torch.arange(K-1, device=y.device).unsqueeze(0).expand(y.size(0), -1)\n    return (y.unsqueeze(1) > ks).float() \n\ndef corn_loss(logits, y, K, pos_weight=None):\n    T = corn_targets(y, K)\n    return F.binary_cross_entropy_with_logits(logits, T, pos_weight=pos_weight)\n\n@torch.no_grad()\ndef corn_predict_logits_to_labels(logits, taus=None):\n    p = torch.sigmoid(logits)            \n    if taus is None:\n        taus = torch.full((logits.size(1),), 0.5, device=logits.device)\n    return (p > taus).sum(dim=1)   \n```\n\n## LSTM\n\n```{python}\n\nclass LSTM(nn.Module):\n  #set up the arch\n  def __init__(self, dynamic_dim , static_dim, output_dim, hidden_dim, num_hidden_layers, rnn_dropout, head_dropout, bidirectional = False):\n    super().__init__()\n   # self.flatten = nn.Flatten()\n\n    self.rnn = nn.LSTM(dynamic_dim, hidden_size=hidden_dim, num_layers=num_hidden_layers, batch_first=True, dropout=(rnn_dropout if num_hidden_layers > 1 else 0.0)\n                       , bidirectional=bidirectional)\n\n  #will have to alter MLP input dim if bidirectional set to True\n\n    self.head = nn.Sequential(\n            nn.Linear(hidden_dim + static_dim, 64),\n            nn.ReLU(), nn.Dropout(head_dropout))#,\n            #nn.Linear(in_features=64, out_features=output_dim))#,\n            #nn.Softmax(dim = 5))\n\n    #self.corn = CORNHead(64, K)\n    #self.K = K\n    \n    # CORN head: K-1 thresholds\n    self.corn = nn.Linear(64, output_dim-1)\n    self.K = K\n\n\n  def forward(self, X_seq, X_static):\n    _, (hiddenStates,_) = self.rnn(X_seq)\n    z = torch.cat((hiddenStates[-1], X_static), dim=1)\n    z = self.head(z)\n    return self.corn(z)\n\n\n```\n\nFold wise imbalance for CORN (fights imbalance in the data). Up weights events that are rare , since our data is imbalanced - higher hazard levels rarer. Calculated as number of responses below K reponse over number above - upweights the rarer classes\nComputes one weight per threshold (response value). Makes rare cases matter more in the BCE loss\n\n```{python}\ndef corn_pos_weight_cb(y_fold, K=5, beta=0.999, device=device):\n    \"\"\"\n    Class-balanced 'effective number' weights for CORN thresholds.\n    For threshold k, positives are (y > k).\n    \"\"\"\n    y_np = y_fold.cpu().numpy()\n    pos = np.array([(y_np > k).sum() for k in range(K-1)], dtype=np.float64)\n    pos = np.clip(pos, 1, None)\n    w = (1 - beta) / (1 - np.power(beta, pos))\n    w = w / w.mean()       # normalize nicely\n    return torch.tensor(w, dtype=torch.float32, device=device)  # (K-1,)\n```\n\nLoads validation fold and evaluates model on it, using ADAM optimiser and CORN for backprop and QWK for validation and early stopping\n\n```{python}\n@torch.no_grad()\ndef eval_corn(model, loader, taus=None):\n    model.eval()\n    ys, ps, loss_sum, n = [], [], 0.0, 0\n    for xseq, xsta, y in loader:\n        xseq, xsta, y = xseq.to(device), xsta.to(device), y.to(device)\n        logits = model(xseq, xsta)\n        loss = corn_loss(logits, y, K)\n        pred  = corn_predict_logits_to_labels(logits, taus=taus.to(device) if taus is not None else None)\n        ys.append(y.cpu().numpy()); ps.append(pred.cpu().numpy())\n        loss_sum += loss.item() * y.size(0); n += y.size(0)\n    y_true = np.concatenate(ys); y_pred = np.concatenate(ps)\n    va_loss = loss_sum / max(n,1)\n    acc = accuracy_score(y_true, y_pred)\n    f1  = f1_score(y_true, y_pred, average=\"macro\")\n    mae = np.abs(y_true - y_pred).mean()\n    try: qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    except: qwk = float(\"nan\")\n    return va_loss, acc, f1, mae, qwk, y_true, y_pred\n\ndef fit_one_corn(model, dl_tr, dl_va, y_tr_fold, lr=2e-3, weight_decay=1e-4,\n                 max_epochs=40, patience=6, head_dropout=0.3):  # patience refers to early stopping epochs\n    model = model.to(device)\n    pos_weight = corn_pos_weight_cb(y_tr_fold, K=K, beta=0.999, device=device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    best_state, best_metric, bad = None, -1e9, 0\n    for ep in range(1, max_epochs+1):\n        model.train()\n        for xseq, xsta, y in dl_tr:\n            xseq, xsta, y = xseq.to(device), xsta.to(device), y.to(device)\n            logits = model(xseq, xsta)\n            loss = corn_loss(logits, y, K, pos_weight=pos_weight)\n            opt.zero_grad(); loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n        # validate (no tuned thresholds yet)\n        va_loss, acc, f1, mae, qwk, _, _ = eval_corn(model, dl_va, taus=None)\n        metric = qwk if np.isfinite(qwk) else (acc - mae)  # ordinal-friendly early stop\n        if metric > best_metric + 1e-4:\n            best_metric, bad = metric, 0\n            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n        else:\n            bad += 1\n            if bad >= patience:\n                break\n    if best_state: model.load_state_dict(best_state)\n    return model\n```\n\nDataloaders\n\nweighted random sampler rwweights which samples appear in batches so minority classes are more common. Has the effect of making bayches contain more underrepresented classes (increased attention on rare events). \n\n```{python}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef make_loaders(Xseq, Xsta, y, tr_idx, va_idx, batch_size):\n    ds = TensorDataset(Xseq, Xsta, y)\n    dl_tr = DataLoader(torch.utils.data.Subset(ds, tr_idx), batch_size=batch_size, shuffle=True)\n    dl_va = DataLoader(torch.utils.data.Subset(ds, va_idx), batch_size=batch_size, shuffle=False)\n    return dl_tr, dl_va\n  \ndef make_sampler_from_labels(y_idxed):\n    counts = torch.bincount(y_idxed.cpu(), minlength=K).float().clamp_min(1)\n    cls_w = (len(y_idxed) / (K * counts)).numpy()\n    sample_w = cls_w[y_idxed.cpu().numpy()]\n    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n\n\ndef loaders_for_fold(tr_idx, va_idx, batch_size=256, oversample=True):\n    ds_tr_all = TensorDataset(Xseq_tr, Xsta_tr, y_tr)\n    y_tr_fold = y_tr[tr_idx]\n    if oversample:\n        sampler = make_sampler_from_labels(y_tr_fold)\n        dl_tr = DataLoader(Subset(ds_tr_all, tr_idx), batch_size=batch_size,\n                           sampler=sampler, drop_last=False)\n    else:\n        dl_tr = DataLoader(Subset(ds_tr_all, tr_idx), batch_size=batch_size,\n                           shuffle=True, drop_last=False)\n    dl_va = DataLoader(Subset(ds_tr_all, va_idx), batch_size=batch_size,\n                       shuffle=False, drop_last=False)\n    return dl_tr, dl_va, y_tr_fold\n```\n\n\n## Hyperparameter tuning\n\n\nDon't Run !!!!\n\n```{python}\nrandom.seed(20)\nimport optuna\n\n# Build folds \nfolds = make_time_folds(tgt_dates_tr, K=5, gap_days= 10)#14)\n\ndef objective(trial):\n    # --- sample hparams ---\n    hidden_dim   = trial.suggest_categorical(\"hidden_dim\", [32, 48, 64])\n    num_layers   = trial.suggest_int(\"num_layers\", 1, 2)\n    rnn_dropout  = trial.suggest_float(\"rnn_dropout\", 0.0, 0.5) if num_layers > 1 else 0.0\n    head_dropout = trial.suggest_float(\"head_dropout\", 0.1, 0.5)\n    lr           = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n    batch_size   = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n\n    scores = []\n    for (tr_idx, va_idx) in folds:            # or folds[-1:] if you only want the latest fold\n        dl_tr, dl_va, y_tr_fold = loaders_for_fold(tr_idx, va_idx,\n                                                   batch_size=batch_size,\n                                                   oversample=True)\n\n        model = LSTM(dynamic_dim=Xseq_tr.shape[-1],\n                                static_dim=Xsta_tr.shape[-1],\n                                output_dim=int(y_tr.max().item())+1,\n                                hidden_dim=hidden_dim,\n                                num_hidden_layers=num_layers,\n                                rnn_dropout=rnn_dropout,\n                                head_dropout=head_dropout,\n                                bidirectional=False)\n\n        # ⬇⬇ pass y_tr_fold here\n        model = fit_one_corn(model, dl_tr, dl_va, y_tr_fold,\n                             lr=lr, weight_decay=weight_decay,\n                             max_epochs=30, patience=5)\n\n        _, acc, f1, mae, qwk, *_ = eval_corn(model, dl_va)  # taus=None during tuning\n        scores.append(qwk if np.isfinite(qwk) else (acc - mae))\n\n    return float(np.mean(scores))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50, show_progress_bar=True)\n\nprint(\"Best params:\", study.best_trial.params)\nprint(\"Best CV score (QWK):\", study.best_value)\n\n```\n\nBest params: {'hidden_dim': 32, 'num_layers': 1, 'head_dropout': 0.3472958534673855, 'lr': 0.0028396801498878554, 'weight_decay': 2.2404755878518988e-06, 'batch_size': 256}\nBest CV score (QWK): 0.6344358948490426\n\nTune CORN Thresholds\n\nNow do small grid search using threholds and final val set (should be most similar to test set) to get final thresholds. Use hyperparameters from early CV to make final model , which will make logits and then probs, but those probs will be converted to classes using these threholds from final val split\n\nbecause test period mix is different (many more 0s), tune the final threholds for the loigit -> predictions step on final val slice , in hopes it most closely resembles the test set\n\n```{python}\nrandom.seed(20)\n#best = study.best_trial.params \n\nbest = {\n  \"lr\": 0.0028396801498878554,\n  \"hidden_dim\": 32,\n  \"num_layers\": 1,\n  \"head_dropout\": 0.3472958534673855,\n  \"weight_decay\": 2.2404755878518988e-06,\n  \"batch_size\": 256\n}\n\n# pick the last fold (most recent dates in train set)\ntr_idx, va_idx = folds[-1]       \n\n# loaders with oversampling - oversample rare hazards\nbatch_size = 256\ndl_tr, dl_va, y_tr_fold = loaders_for_fold(tr_idx, va_idx, batch_size=batch_size, oversample=True)\n\n# fit model\ndyn_dim = Xseq_tr.shape[-1]; sta_dim = Xsta_tr.shape[-1]\nmodel = LSTM(dyn_dim, sta_dim, output_dim=K, hidden_dim=48, num_hidden_layers=best[\"num_layers\"], rnn_dropout= 0, head_dropout=best[\"head_dropout\"]) #best[\"rnn_dropout\"]\n\n# train on latest fold\nmodel = fit_one_corn(model, dl_tr, dl_va, y_tr_fold, lr= best[\"lr\"], weight_decay=best[\"weight_decay\"], max_epochs=50, patience=8)\n\n# collect validation probabilities\n@torch.no_grad()\ndef collect_probs_and_labels(model, loader):\n    model.eval()\n    probs, ys = [], []\n    for xseq, xsta, y in loader:\n        xseq, xsta = xseq.to(device), xsta.to(device)\n        logits = model(xseq, xsta)\n        probs.append(torch.sigmoid(logits).cpu().numpy())  # (B, K-1)\n        ys.append(y.cpu().numpy())\n    return np.vstack(probs), np.concatenate(ys)\n\nprobs_val, y_val = collect_probs_and_labels(model, dl_va)\n```\n\nTune monotone thresholds on latest fold \nthe threholds are monotone - meaning that for the lower classes they must be greater than or equal to those decision boundaries/threholds for the higher classes (so higher classes aren't easier to cross than lower ones)\n\n```{python}\nrandom.seed(20)\nfrom sklearn.metrics import cohen_kappa_score\n\ndef tune_corn_thresholds(probs_val, y_val, grid=np.linspace(0.3, 0.7, 21)):\n    Km1 = probs_val.shape[1]\n    taus = [0.5]*Km1\n    \n    for k in range(Km1):\n        best_tau, best_q = taus[k], -1\n        for t in grid:\n            cand = taus.copy()\n            cand[k] = float(t)\n          \n            for j in range(Km1-2, -1, -1):\n                cand[j] = max(cand[j], cand[j+1])\n            y_pred = (probs_val > np.array(cand)).sum(axis=1)\n            q = cohen_kappa_score(y_val, y_pred, weights=\"quadratic\")\n            if q > best_q:\n                best_q, best_tau = q, cand[k]\n        taus[k] = best_tau\n    taus = np.array(taus, dtype=np.float32)\n    final_q = cohen_kappa_score(y_val, (probs_val > taus).sum(axis=1), weights=\"quadratic\")\n    return taus, final_q\n\ntaus, q_val = tune_corn_thresholds(probs_val, y_val)\nprint(\"Tuned taus (monotone):\", np.round(taus, 3), \" | Val QWK with taus:\", round(q_val, 3))\n\n```\n\nOutput here are the thresholds used for the different classes\n\nTest set evaluation with thresholds (for converting logits to labels 0..4 from final fold - more representative of test set)\n\n```{python}\nrandom.seed(20)\n# Retrain on *all* training windows (use the same hparams you selected on the latest fold)\nds_all = TensorDataset(Xseq_tr, Xsta_tr, y_tr)\n# Oversample for all-train too:\nsampler_all = make_sampler_from_labels(y_tr)\ndl_all = DataLoader(ds_all, batch_size=batch_size, sampler=sampler_all, drop_last=False)\n\n# simple early stopping on the last 10% time slice of train to avoid overfit\nN = Xseq_tr.size(0); split = int(N*0.9)\ndl_tr_full = DataLoader(TensorDataset(Xseq_tr[:split], Xsta_tr[:split], y_tr[:split]),\n                        batch_size=batch_size, sampler=make_sampler_from_labels(y_tr[:split]))\ndl_va_full = DataLoader(TensorDataset(Xseq_tr[split:], Xsta_tr[split:], y_tr[split:]),\n                        batch_size=batch_size, shuffle=False)\n\n#do final model fit with best parameters, with small validation tail to detect overfitting\nmodel_full = LSTM(dyn_dim, sta_dim, output_dim=K, hidden_dim=48, num_hidden_layers=1, rnn_dropout=0, head_dropout=best[\"head_dropout\"]) #best[\"rnn_dropout\"]\nmodel_full = fit_one_corn(model_full, dl_tr_full, dl_va_full, y_tr[:split],\n                          lr=best[\"lr\"], weight_decay=best[\"weight_decay\"], max_epochs=50, patience=8)\n\n# Evaluate Test using the thresholds (taus) we found on the last fold - most similar to test set\ndl_te = DataLoader(TensorDataset(Xseq_te, Xsta_te, y_te), batch_size=batch_size, shuffle=False)\n\n@torch.no_grad()\ndef eval_test_with_taus(model, loader, taus_np):\n    taus_t = torch.tensor(taus_np, dtype=torch.float32, device=device)\n    model.eval()\n    ys, ps, loss_sum, n = [], [], 0.0, 0\n    for xseq, xsta, y in loader:\n        xseq, xsta, y = xseq.to(device), xsta.to(device), y.to(device)\n        logits = model(xseq, xsta)\n        loss = corn_loss(logits, y, K)\n        pred  = corn_predict_logits_to_labels(logits, taus=taus_t)\n        ys.append(y.cpu().numpy()); ps.append(pred.cpu().numpy())\n        loss_sum += loss.item() * y.size(0); n += y.size(0)\n    y_true = np.concatenate(ys); y_pred = np.concatenate(ps)\n    te_loss = loss_sum / max(n,1)\n    acc = accuracy_score(y_true, y_pred)\n    f1  = f1_score(y_true, y_pred, average=\"macro\")\n    mae = np.abs(y_true - y_pred).mean()\n    try: qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    except: qwk = float(\"nan\")\n    return te_loss, acc, f1, mae, qwk\n\nte_loss, te_acc, te_f1, te_mae, te_qwk = eval_test_with_taus(model_full, dl_te, taus)\nprint(f\"TEST | loss {te_loss:.3f} acc {te_acc:.3f} f1 {te_f1:.3f} mae {te_mae:.3f} qwk {te_qwk:.3f}\")\n\n```\nTEST | loss 0.266 acc 0.624 f1 0.326 mae 0.442 qwk 0.482\n\nhigher QWK with similar or lower MAE, better calibrated, fewer big jumps\n\nConfusion Matrix\n\n```{python}\nrandom.seed(20)\nimport torch, numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score\n\nK = 5  # classes 0..4\n\n# collect logits on test\nmodel.eval()\nlogits_list, y_list = [], []\nwith torch.no_grad():\n    for xseq, xsta, y in dl_te:  # your test DataLoader\n        xseq, xsta = xseq.to(device), xsta.to(device)\n        logits = model(xseq, xsta)           # (B, K-1)\n        logits_list.append(logits.cpu())\n        y_list.append(y.cpu())\ny_true    = torch.cat(y_list).numpy()\n\n# after inference on test\nwith torch.no_grad():\n    logits_te = torch.cat([model(x.to(device), s.to(device)).cpu()\n                           for x,s,_ in dl_te])\nprobs_te = torch.sigmoid(logits_te).numpy()     # (N, K-1)\n\ntaus = np.array([0.52, 0.50, 0.50, 0.48], dtype=np.float32)   # from your tuning\ny_pred = (probs_te > taus).sum(axis=1)          # <- use taus here\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score\n\nlabels = [0,1,2,3,4]  # FAH levels\n\ncm = confusion_matrix(y_true, y_pred, labels=labels)\nprint(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n\n# Per-class precision/recall/F1 and macro/weighted averages:\nprint(\"\\nPer-class report:\")\nprint(classification_report(y_true, y_pred, labels=labels, digits=3))\n\n# If you just want the recall vector (per class):\nrecall_per_class = recall_score(y_true, y_pred, labels=labels, average=None)\nprint(\"Recall per class:\", recall_per_class)\n```\n\n\n# Majority Baseline\n\npredicts most frequent class from training set\nhave a global prediction (most common hazard forecast over whole set vs most common per area)\n\n```{python}\nrandom.seed(20)\nimport numpy as np, pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n\n# Global majority from TRAIN labels\nmajor = train[\"FAH_ord\"].mode().iloc[0]\ny_pred_major = np.full(len(test), major)\n\n# Per-Area majority (a stronger baseline)\narea_cols = [c for c in train.columns if c.startswith(\"Area_\")]\ndef area_name(df):\n    idx = df[area_cols].values.argmax(axis=1)\n    return np.array([area_cols[i].replace(\"Area_\",\"\") for i in idx])\n\ntrain[\"AreaName\"] = area_name(train)\ntest[\"AreaName\"]  = area_name(test)\nper_area_major = (train.groupby(\"AreaName\")[\"FAH_ord\"]\n                        .agg(lambda s: s.mode().iloc[0]))\ny_pred_area_major = test[\"AreaName\"].map(per_area_major).fillna(major).to_numpy()\n\ndef report(y_true, y_pred, label):\n    acc  = accuracy_score(y_true, y_pred)\n    f1   = f1_score(y_true, y_pred, average=\"macro\")\n    mae  = np.abs(y_true - y_pred).mean()\n    qwk  = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    print(f\"{label:18s} | acc {acc:.3f}  macroF1 {f1:.3f}  MAE {mae:.3f}  QWK {qwk:.3f}\")\n\nreport(test[\"FAH_ord\"].to_numpy(), y_pred_major,      \"Majority (global)\")\nreport(test[\"FAH_ord\"].to_numpy(), y_pred_area_major, \"Majority (per-area)\")\n```\n\nMajority (global)  | acc 0.305  macroF1 0.094  MAE 0.735  QWK 0.000\nMajority (per-area) | acc 0.393  macroF1 0.200  MAE 0.755  QWK 0.090\n\n\n# Persistence Baseline \n\nPredicts todays hazard as the same as previous day\n\n```{python}\nrandom.seed(20)\n# Build previous-day target per (Area, Date)\ntest_sorted = t#est.sort_values([\"AreaName\",\"Date\"]).copy()\ntrain_sorted= train.sort_values([\"AreaName\",\"Date\"]).copy()\n\n# concat to allow the first test day to look back into the end of train\nall_ = pd.concat([train_sorted, test_sorted], ignore_index=True)\n\n# y_prev(t) = y(t-1) within the same Area\nall_[\"y_prev\"] = (all_.groupby(\"AreaName\")[\"FAH_ord\"]\n                     .shift(1))  # NaN for each area's first row\n\n# Extract predictions for test rows only\npersist_pred = all_.loc[all_.index.isin(test_sorted.index), \"y_prev\"].to_numpy()\n\n# Fallback for the very first day of each area in test (no previous)\n# use that area's majority or global majority\nmask = np.isnan(persist_pred)\nfallback = test_sorted.loc[mask, \"AreaName\"].map(per_area_major).fillna(major).to_numpy()\npersist_pred[mask] = fallback\n\nreport(test_sorted[\"FAH_ord\"].to_numpy(), persist_pred.astype(int), \"Persistence (y[t-1])\")\n```\n\nPersistence (y[t-1]) | acc 0.203  macroF1 0.139  MAE 1.453  QWK -0.041\n\nDiscrepancy between the majority and persistence baselines, and the model's performnace suggests it is better than a naive approach (is learning)\n\n","srcMarkdownNoYaml":"\n\n```{r}\nlibrary(reticulate)\nuse_condaenv(\"ds4i-mixed\", required = TRUE)\npy_config()\n```\n\n\n```{r echo=F, message = F, warning = F}\nset.seed(5073)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(naniar)\nlibrary(janitor)\nlibrary(tidymodels)\nlibrary(rpart)\nlibrary(recipes)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(themis)\n\n# read in the data\naval <- read.csv(\"scotland_avalanche_forecasts_2009_2025.csv\")\n```\n\n\n\n```{r echo=F}\n# quick checks\n#colnames(aval)\n#colSums(is.na(aval)) # number of missing values in each column\n#str(aval) # data type of each column\n\n# remove OAH: observed avalanche hazard\naval <- aval %>% select(-OAH)\n```\n\n# Data preparation: variable types\nWe begin by doing the following:\n1. Change the 'Date' variable to a Date class and extract the year, month and day of year (doy) as new variables.\n2. Create a new variable called 'Season' that groups the months together into seasons\n3. Ensure text variables are character classes and that indicators are factors\n4. Create a new variable for the response called FAH_ord, that casts FAH as a factor and assigns the different values to ordered levels\n5. Convert all the wind direction and aspect variables to their sine and cosine versions so that degrees that are far apart numerically but close together geographically would be close together numerically as well (ex. 0 degrees and 350 degrees).\n\n```{r echo=F}\n# clean the data by casting variables as correct types\n# STEP 1 — Fix types & engineer helper features (with OAH removed)\n\naval <- aval %>%\n  mutate(\n    # --- time features ---\n    DateTime = ymd_hms(Date, quiet = TRUE),\n    # strip out year, month and day:\n    Date     = as.Date(DateTime),\n    year     = year(Date),\n    month    = month(Date),\n    doy      = yday(Date),\n    # create a new variable: season\n    season   = factor(case_when(\n      month %in% c(12,1,2) ~ \"DJF\",\n      month %in% c(3,4,5)  ~ \"MAM\",\n      month %in% c(6,7,8)  ~ \"JJA\",\n      TRUE                 ~ \"SON\"\n    ), levels = c(\"DJF\",\"MAM\",\"JJA\",\"SON\")),\n\n    # --- categorical / IDs ---\n    Area     = factor(Area),\n    OSgrid   = as.character(OSgrid),\n    Location = as.character(Location),\n    Obs      = factor(Obs),\n    Drift = factor(Drift),\n    Rain.at.900 = factor(Rain.at.900),\n\n    # --- hazard (target) as ordered factor ---\n    FAH_ord  = factor(\n      FAH,\n      # specify the following order:\n      levels  = c(\"Low\",\"Moderate\",\"Considerable -\",\"Considerable +\",\"High\"),\n      ordered = TRUE\n    )\n  )\n```\n\nSome specific questions regarding the data:\n1. Is longitude and latitude constant within OSgrid?\n2. Is Alt constant within OSgrid?\n\n```{r echo=F}\n# is long and lat constant within OSgrid?\naval %>%\n  group_by(OSgrid) %>%\n  summarise(n_coords = n_distinct(paste(longitude, latitude)), .groups = \"drop\") %>%\n  filter(n_coords > 1)\n# yes, thus long and lat is the coordinates of different sites, OSgrid\n\n# is Alt the same within each OSgrid?\naval %>%\n  group_by(OSgrid) %>%\n  summarise(n_alt = n_distinct(Alt), .groups = \"drop\") %>%\n  filter(n_alt > 1)\n## no: cannot use Osgrid to impute missing Alt\n```\n\n# Investigate duplicates\n\n```{r}\n# should we add area, i.e. do multiple areas exist per (Date, OSgrid)\naval %>%\n  count(Date, OSgrid, Area) %>%\n  count(Date, OSgrid) %>%\n  filter(n > 1)  # there are some, so add area.\n\naval %>% \n  add_count(Date, OSgrid, Area, name=\"dup_n\") %>% \n  filter(dup_n > 1)\n```\n\n\nSome of these rows don't just differ by NA, but have conflicting numeric values. For these, keep both records, but for those that only differ by an NA, collapse the rows to keep the most numeric values per column.\n\n```{r}\n## 0) Define and validate keys -----------------------------------------------\nkey_cols <- c(\"Date\", \"OSgrid\", \"Area\")\nmissing_keys <- setdiff(key_cols, names(aval))\nif (length(missing_keys)) {\n  stop(\"These key columns are not in `aval`: \", paste(missing_keys, collapse = \", \"))\n}\n\n## 1) Build candidate non-key columns once (and be explicit) -----------------\nnon_key_cols <- setdiff(names(aval), key_cols)\n\n## 2) Find conflicts in ANY column type (numeric OR categorical) -------------\n# - use any_of() so it won't error if something is off\n# - n_distinct on na.omit for each column within the key\nwide_conflicts <- aval %>%\n  group_by(across(all_of(key_cols))) %>%\n  summarise(\n    across(all_of(non_key_cols), ~ n_distinct(na.omit(.x)), .names = \"nuniq_{.col}\"),\n    .groups = \"drop\"\n  ) %>%\n  # mark groups where any column has >1 distinct observed value\n  mutate(any_conflict = if_any(starts_with(\"nuniq_\"), ~ .x > 1)) %>%\n  filter(any_conflict) %>%\n  select(all_of(key_cols)) %>%\n  distinct()\n\n## 3) Collapse only conflict-free groups -------------------------------------\n# typed NA + first_non_na for safe collapsing\n\nfirst_non_na <- function(x) {\n  i <- which(!is.na(x))[1]\n  if (is.na(i)) x[NA_integer_] else x[i]\n}\n\ncollapsed_ok <- aval %>%\n  anti_join(wide_conflicts, by = key_cols) %>%\n  group_by(across(all_of(key_cols))) %>%\n  summarise(\n    across(all_of(non_key_cols), first_non_na),\n    .rows_collapsed = dplyr::n(),\n    .collapsed = TRUE,\n    .groups = \"drop\"\n  )\n\n## 4) Keep conflicting groups as-is (flag them) -------------------------------\nkept_conflicts <- aval %>%\n  semi_join(wide_conflicts, by = key_cols) %>%\n  mutate(.collapsed = FALSE)\n\n## 5) Combine and (optionally) drop flags ------------------------------------\naval_dups_resolved <- bind_rows(collapsed_ok, kept_conflicts) %>%\n  arrange(across(all_of(key_cols)))\n\naval <- aval_dups_resolved %>% select(-.collapsed, -.rows_collapsed)\n```\n\n\n# Investigate missing data\n\n## Visualise missingness\nDetermine the percentage missing values per variable and visualise this.\n\n```{r echo=F}\n# Per-variable % missing\nmiss_summary <- aval %>%\n  summarise(across(\n    everything(),\n    ~mean(is.na(.)) * 100\n  )) %>%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"pct_missing\") %>%\n  arrange(desc(pct_missing))\n\nmiss_summary\n\n# Visual overview\nnaniar::gg_miss_var(aval)\nnaniar::vis_miss(aval)\n```\n\n## Strategy for missing values\n\nFrom the above, only 10 variables have more than 5% of their values missing. Careful attention is paid to these below. The remaining variables have $<5\\%$ missing values and will simply be imputed with a Bagged-tree imputation approach.\n\nOf the variables missing more than 5%, we first need to determine if the missingness carries meaning. The list of variables is thus split in two, one where missingness does carry meaning and need to be accounted for and one where it does not.\n\nThe variables that do need to be accounted for are:\n- **`AV.Cat` (23.4%)**: Missing avalanche category values likely mean that no category was assigned for that day. Forecasters usually provide a category when avalanches are observed or when conditions are clear enough to classify. If it is missing, that could itself indicate that avalanches were not observed or that conditions were uncertain, which is meaningful information about overall stability.  \n\n- **`Ski.Pen` (22.5%)**: Ski penetration is only recorded when conditions allow observers to ski on the slope. If this field is missing, it often means the snow was too hard, too shallow, or otherwise unsuitable for skiing. This absence therefore reflects snow surface properties that can be related to avalanche hazard.  \n\n- **`Crystals` (9.3%)**: Crystal type is identified through snow-pit observations. Missing values here often mean that no pit was dug on that day, which in turn may depend on perceived stability, time constraints, or safety concerns. Thus, the lack of a crystal observation can itself provide information about conditions.  \n\n- **`Wetness` (5.4%)**: Wetness is typically noted when meltwater or damp snow is present. If this field is missing, it may indicate that the snow was dry and that observers did not see a reason to record wetness. Hence, missingness can indirectly point to dry-snow conditions.  \n\n- **`Snow.Index` (7.0%)**: This is a derived stability metric based on snowpack tests. If the value is missing, it likely means the relevant tests were not carried out, perhaps because conditions didn’t warrant them. This absence can therefore reflect judgments about snow stability.  \n\n- **`Summit.Wind.Dir_sin / Summit.Wind.Dir_cos` (12.4%), `Summit.Wind.Speed` (8.5%), and `Summit.Air.Temp` (7.1%)**: Missing summit weather variables may not just be sensor errors. It is plausible that readings were unavailable because weather at the summit was too extreme or dangerous for measurement, such as during storms or blizzards. In that case, missingness itself could be linked to hazardous conditions.  \n\nFor each of these, an indicator will be created to show if the value was missing.\n\nThose variables that do not need explicit missingness indicators are:\n\n- **`Max.Temp.Grad` (6.7%)**: This variable reflects temperature gradients measured in snow-pit tests. When missing, it is usually because the snow-pit test was not performed. However, the decision not to perform a pit is already captured by other variables where missingness is more clearly informative (e.g. `Crystals`, `Snow.Index`). Adding another indicator here would add redundancy without extra insight. The values themselves will be imputed with KNN.  \n\n- **`Max.Hardness.Grad` (5.9%)**: Like `Max.Temp.Grad`, hardness gradients are only measured in pits. Missing values again overlap with the same “pit not performed” scenario already captured by other indicators. For this reason, a separate indicator is unnecessary. The variable will be imputed with KNN to fill the missing numeric values.  \n\nAfter the relevant indicators are created, all remaining missing values will be imputed with **Bagged tree imputation**. Bagged tree imputation is a machine-learning approach where missing values in a variable are predicted using an ensemble of decision trees fit on the observed cases. Each tree is trained on a bootstrap sample of the data, and predictions are averaged across trees to produce stable and robust imputations. Unlike simple mean/median imputation or KNN, bagged trees can capture non-linear relationships and interactions among predictors, making them well suited to complex, structured data. \n\nIn the avalanche dataset, where variables combine topography, weather, and snowpack characteristics, and missingness can depend on multiple interacting factors, bagged tree imputation offers a principled way to exploit those dependencies while limiting noise from any single predictor. This allows us to fill gaps more realistically while preserving the multivariate structure that is important for downstream modeling with neural networks.\n\n\n```{r echo = F}\n# --- (A) Ensure target exists & indicators (0/1) are present ---\naval <- aval %>%\n  mutate(\n    av_cat_missing_initial            = as.integer(is.na(AV.Cat)),\n    ski_pen_missing_initial           = as.integer(is.na(Ski.Pen)),\n    crystals_missing_initial          = as.integer(is.na(Crystals)),\n    wetness_missing_initial           = as.integer(is.na(Wetness)),\n    snow_index_missing_initial        = as.integer(is.na(Snow.Index)),\n    summit_wind_dir_missing_initial   = as.integer(is.na(Summit.Wind.Dir)),\n    summit_wind_speed_missing_initial = as.integer(is.na(Summit.Wind.Speed)),\n    summit_air_temp_missing_initial   = as.integer(is.na(Summit.Air.Temp))\n  )\n\n```\n\nThe imputation will be done at a later stage. At first, the data still needs to be cleaned. Note that these missing values were intentionally set now before looking at improbable or outlier values below that are then encoded as NA. The reason for this is to truly only capture \"meaningful\" missingness in these indicators and not convolute them with missingness due to incorrect values being entered.\n\n# Cleaning of continuous variables\n\n```{r}\nnums <- names(dplyr::select(aval, where(is.numeric)))\nn_tot <- nrow(aval)\n\ncont_summary <- tibble(variable = nums) %>%\n  rowwise() %>%\n  mutate(\n    n_nonmiss   = sum(!is.na(aval[[variable]])),\n    n_missing   = n_tot - n_nonmiss,\n    pct_missing = round(100 * n_missing / n_tot, 2),\n    mean   = if (n_nonmiss > 0) mean(aval[[variable]], na.rm = TRUE)   else NA_real_,\n    median = if (n_nonmiss > 0) median(aval[[variable]], na.rm = TRUE) else NA_real_,\n    min    = if (n_nonmiss > 0) min(aval[[variable]], na.rm = TRUE)    else NA_real_,\n    max    = if (n_nonmiss > 0) max(aval[[variable]], na.rm = TRUE)    else NA_real_,\n    sd     = if (n_nonmiss > 1) sd(aval[[variable]],  na.rm = TRUE)    else NA_real_\n  ) %>%\n  ungroup() %>%\n  arrange(desc(pct_missing), variable)\n\ncont_summary\n```\n\n### Resonable variables\nThe following variables seemed reasonable in terms of outliers or impossible values:\n1. AV.Cat\n2. Crystals\n3. Summit.Air.Temp\n4. Max.Hardness.Grad\n5. Air Temperature\n6. Summit.Wind.Speed (assume wind speeds are in km/h)\n7. Wind.Speed (assume wind speeds are in km/h)\n\nThe remaining numerical variables were all cleaned.\n\n\n### Ski Penetration\n\n```{r}\nhist(aval$Ski.Pen)\nlength(which(aval$Ski.Pen<0))\n\n# remove all values <0:\naval$Ski.Pen[aval$Ski.Pen<0] <- NA_real_\n```\n\n### Snow Index\nSnow.Index is a derived stability score based on field observations such as penetration tests, hardness gradients, and crystal type. \n\nInitial suspicion: At first glance, the distribution of Snow.Index raised concerns because the vast majority of observations (over 7,700 of 10,671) had a value of zero, with only 57 unique values overall. Given the large number of missing entries in related snowpit variables, this pattern suggested that a zero might be functioning as a placeholder for “no measurement taken,” rather than a genuine stability score.\n\n```{r}\n# unique(aval$Snow.Index)\n# length(unique(aval$Snow.Index)) #57\nhist(aval$Snow.Index, breaks = 30)\n\n# length(which(aval$Snow.Index>100)) # 28\n# length(which(aval$Snow.Index>50)) # 35\n# length(which(aval$Snow.Index<0)) # 70\n# length(which(aval$Snow.Index>0 & aval$Snow.Index<20)) # 2021\n# length(which(aval$Snow.Index==0)) # 7770 actually equals 0\n```\n\nLet's try to confirm this suspicion by doing some checks. The following table looks at the distribution of values for other variables whre Snow.Index is zero.\n\n```{r}\n# Which rows have Snow.Index == 0 ?\nidx_zero <- aval$Snow.Index == 0\n\n# Compare % missing in other key variables for zero vs non-zero Snow.Index\naval %>%\n  mutate(group = case_when(\n    Snow.Index == 0 ~ \"Zero\",\n    is.na(Snow.Index) ~ \"NA\",\n    TRUE ~ \"Non-zero\"\n  )) %>%\n  summarise(\n    across(c(Ski.Pen, Foot.Pen, Crystals, Wetness, Max.Temp.Grad, Max.Hardness.Grad),\n           ~ mean(is.na(.)), \n           .names = \"{.col}_missing_rate\"),\n    n = n(),\n    .by = group\n  )\n```\n\nFurther inspection: However, when the missingness patterns were examined, rows with a Snow.Index of zero showed very low missingness in other snowpit variables (typically less than 10%). This indicates that these zeros are not simply standing in for absent measurements. Instead, they appear to represent true observations where no instability was detected. On this basis, zeros were retained as valid values, and only genuine missing entries (NA) were flagged for imputation.\n\n\n\n\n### Wetness\n\n```{r}\nhist(aval$Wetness)\nunique(aval$Wetness) # looks more like an index, keep as is.\n# length(which(aval$Wetness>9))\n```\n\n### Insolation\nInsolation measures the incoming solar radiation at the observation site, here recorded on a coded scale ranging mostly from 0–20. This index reflects sunlight exposure, which plays an important role in snowpack warming and wet-snow avalanche activity. A small number of observations fall outside the expected range, including negative values and extreme outliers above 20 (e.g., -55, 106, 208). Since these are not physically meaningful, they were treated as data entry errors and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Insolation)\nunique(aval$Insolation)\ntable(aval$Insolation)\n\n# encode <0 and >20 as NA\naval$Insolation[which(aval$Insolation<0 |\n                        aval$Insolation>20)] <- NA_real_\n```\n\n### Snow Temperature\n\nSnow.Temp records the snowpack temperature in degrees Celsius. Values in this dataset range from -13 to 124 °C. Since snow cannot persist above 0 °C, values above 5 °C were considered physically implausible and were recoded as missing for later imputation. Negative values down to -13 °C were retained as realistic cold-snow conditions.\n\n```{r}\nhist(aval$Snow.Temp)\nsort(unique(aval$Snow.Temp))\nlength(which(aval$Snow.Temp>5)) # 176\nlength(which(aval$Snow.Temp<10)) # 10127\n\n# make all values above 5 NA\naval$Snow.Temp[which(aval$Snow.Temp>5)] <- NA_real_\n```\n\n### Aspect\n\nAspect describes the compass direction of the slope, measured in degrees clockwise from north (0–360°). It influences how much solar radiation a slope receives and therefore affects snow metamorphism and avalanche risk. In the dataset, any values outside the valid range of 0–360° were considered invalid and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Aspect)\nlength(which(aval$Aspect>360)) # only 8- make these NA\nlength(which(aval$Aspect<0)) # only 4- make these NA\n\n# make values >360 and <0 NA:\naval$Aspect[which(aval$Aspect<0 |\n                    aval$Aspect>360)] <- NA_real_\n```\n\n### No.Settle\n\n```{r}\nhist(aval$No.Settle) # looks fine\n```\n\n\n\n\n### Total Snow Depth\nTotal Snow depth represents the measured thickness of the snowpack in centimeters. Most values lie between 0–500 cm, which is consistent with expected conditions in the observation areas. Negative values are not physically possible, while a small number of extreme outliers above 500 cm were judged unrealistic for the region. These implausible entries were recoded as missing for later imputation.\n\n```{r}\nhist(aval$Total.Snow.Depth)\nlength(which(aval$Total.Snow.Depth>500)) # only 14\nlength(which(aval$Total.Snow.Depth<0)) # only 38\n\naval$Total.Snow.Depth[which(aval$Total.Snow.Depth>500|\n                              aval$Total.Snow.Depth<0)] <- NA_real_\n```\n\n\n### Wind Direction and Summit Wind Direction\nWind direction values less than 0° or greater than 360° are not physically possible. Since the source of these errors is unknown, they were treated as missing (NA) rather than corrected by assumption. These missing values will later be imputed using bagged-tree models, with a corresponding missingness indicator retained to capture any systematic patterns in measurement failure.\n\n```{r}\nhist(aval$Summit.Wind.Dir) # clear outliers/ noise\nhist(aval$Wind.Dir) # clear outliers/ noise\n# length(which(aval$Summit.Wind.Dir>360)) # 4\n# length(which(aval$Summit.Wind.Dir<0)) # 191\n# length(which(aval$Wind.Dir>360)) # 1\n# length(which(aval$Wind.Dir<0)) # 13\n\n# assign NA to these and the cos and sin versions:\n# 1) Identify invalid entries (out of [0, 360])\nidx_wind_invalid   <- !is.na(aval$Wind.Dir)        & (aval$Wind.Dir < 0 | aval$Wind.Dir > 360)\nidx_summit_invalid <- !is.na(aval$Summit.Wind.Dir) & (aval$Summit.Wind.Dir < 0 | aval$Summit.Wind.Dir > 360)\n# sum(idx_wind_invalid) # 14\n# sum(idx_summit_invalid) # 195\n\n# 2) Set invalid directions to NA\naval$Wind.Dir[idx_wind_invalid]               <- NA_real_\naval$Summit.Wind.Dir[idx_summit_invalid]      <- NA_real_\n```\n\n\n### Foot Penetration\nFoot penetration measures the depth in centimeters that an observer’s boot sinks into the snow surface. It reflects snow hardness and helps assess surface stability. Typical values range from a few centimeters in firm snow to over a meter in very soft conditions. In the dataset, negative values and extreme entries above 100 cm were judged implausible and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Foot.Pen)\nlength(which(aval$Foot.Pen>100)) # only 3\nlength(which(aval$Foot.Pen<0)) # only 4\n\naval$Foot.Pen[which(aval$Foot.Pen<0|\n                      aval$Foot.Pen>100)] <- NA_real_\n```\n\n### Incline\nIncline represents the slope angle at the observation site, recorded in degrees. Since slope angle is central to avalanche release, accurate measurement is critical. Values should range from 0° (flat) to 90° (vertical). In the dataset, negative values and outliers above 90° were considered invalid and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Incline)\nlength(which(aval$Incline>90)) # only 4\nlength(which(aval$Incline<0)) # only 3\n\naval$Incline[which(aval$Incline<0|\n                     aval$Incline>90)] <- NA_real_\n```\n\n### Cloud\n\nCloud represents the observed cloud cover, recorded as a percentage from 0 (clear sky) to 100 (fully overcast). This variable provides important context for snowpack conditions, since cloud cover influences surface cooling, radiation balance, and melting. In the dataset, most values fell within the expected 0–100% range. A small number of impossible values (e.g., -1) were considered data entry errors and recoded as missing for later imputation.\n\n```{r}\nhist(aval$Cloud)\nlength(which(aval$Cloud > 100)) # only 1\nlength(which(aval$Cloud < 0))   # only 3\n\naval$Cloud[which(aval$Cloud < 0 |\n                   aval$Cloud > 100)] <- NA_real_\n```\n\n\n### Maximum temperature grading\nIn avalanche datasets, Max.Temp.Grad = Maximum Temperature Gradient within the snowpack profile.When snow profiles are dug, forecasters often measure snow temperature at different depths (surface, mid-pack, base). They then compute the temperature gradient (°C per 10 cm) between layers. The maximum gradient across all measured layers is recorded as Max.Temp.Grad.\n\nIn general, we would expect values between 0-5 to be very common, 5-10 to be less common, but plausible and values of greater than 10 to be very rare.  Any value greater than 10 was assumed to be an error and was replaced with an NA value.\n\n```{r}\nhist(aval$Max.Temp.Grad) # clear outliers/ noise, a value of 130 is physically impossible\nlength(which(aval$Max.Temp.Grad>10)) # 160 \n\naval$Max.Temp.Grad[which(aval$Max.Temp.Grad>10)] <- NA_real_\n```\n\n\n### Altitude\n\nAltitude values greater than 1400 m or less than 0 m are physically impossible within the Scottish study area (highest peak 1345 m). These were treated as missing for later imputation.\n\n```{r}\nhist(aval$Alt[aval$Alt<2000])\n# length(which(aval$Alt>10000)) # only 2\n\naval$Alt[aval$Alt < 0 | aval$Alt > 1400] <- NA_real_\n```\n\n\n```{r}\n# sum(is.na(aval$Alt)) # now 9 are missing\n```\n\nAfter replacing impossible values with NA, 9 observations had missing Altitude. Longitude and latitude were found to be constant within each OSgrid, i.e. each OSgrid corresponds to a unique coordinate pair. However, altitude was not unique within these coordinates, suggesting that the OSgrid represents an area grid of unknown size, while the longitude and latitude are specific sampling points within that grid.\n\nTo impute missing Altitude values, an open-source elevation API will be queried at the recorded longitude and latitude of each observation. This approach assumes that altitude variation within the grid is limited, and that the elevation at the given coordinates is representative for the corresponding observation. \n\nWhen doing this, it was noticed that one of the altitudes was zero. Upon further investigation, the point was found to be in Loch Fyne (Sea Loch), and was thus assumed to be an erroroneous longitude and latitude.\n\n```{r}\n# --- 1) Helper: call Open-Elevation for a batch of lat/lon points ---\n# Expects a tibble/data.frame with columns: latitude, longitude\n# Returns: tibble(latitude, longitude, elev_m)\noe_lookup_batch <- function(points_df) {\n  if (nrow(points_df) == 0) return(tibble(latitude = numeric(), longitude = numeric(), elev_m = numeric()))\n  # Build \"lat,lon|lat,lon|...\" string\n  locs <- points_df %>%\n    transmute(pair = sprintf(\"%.6f,%.6f\", latitude, longitude)) %>%\n    pull(pair) %>%\n    paste(collapse = \"|\")\n\n  url <- paste0(\"https://api.open-elevation.com/api/v1/lookup?locations=\", URLencode(locs))\n\n  resp <- request(url) |> req_timeout(30) |> req_perform()\n\n  if (resp_status(resp) != 200) {\n    warning(\"Open-Elevation request failed with status: \", resp_status(resp))\n    return(tibble(latitude = numeric(), longitude = numeric(), elev_m = numeric()))\n  }\n\n  dat <- resp_body_json(resp, simplifyVector = TRUE)\n  res <- as_tibble(dat$results)\n  # API returns 'elevation' (meters), 'latitude', 'longitude'\n  res %>%\n    transmute(\n      latitude  = as.numeric(latitude),\n      longitude = as.numeric(longitude),\n      elev_m    = as.numeric(elevation)\n    )\n}\n\n# --- 2) Wrapper: batch over many points, with simple rate limiting & safety ---\noe_lookup <- function(points_df, batch_size = 80, sleep_secs = 1) {\n  # round coords to reduce accidental duplicates/float precision issues\n  pts <- points_df %>%\n    transmute(\n      latitude  = round(as.numeric(latitude), 6),\n      longitude = round(as.numeric(longitude), 6)\n    ) %>%\n    distinct()\n\n  batches <- split(pts, ceiling(seq_len(nrow(pts)) / batch_size))\n\n  safe_batch <- safely(oe_lookup_batch, otherwise = tibble(latitude=numeric(), longitude=numeric(), elev_m=numeric()))\n  results <- map(batches, function(b) {\n    out <- safe_batch(b)\n    Sys.sleep(sleep_secs)\n    if (!is.null(out$error)) warning(\"Batch failed: \", out$error)\n    out$result\n  })\n\n  bind_rows(results)\n}\n\n# --- 3) Apply to your data: only rows with missing Alt ---\n# Assumes your data frame is named `aval` and has columns Alt, latitude, longitude\nto_fill <- aval %>%\n  filter(is.na(Alt)) %>%\n  transmute(latitude = latitude, longitude = longitude)\n\nelev_tbl <- oe_lookup(to_fill, batch_size = 80, sleep_secs = 1)\n\n# --- 4) Join back and fill Alt where missing ---\n# 1) Build a lookup key on the API results (rounded to match request precision)\nelev_lu <- elev_tbl %>%\n  mutate(key = paste0(round(latitude, 6), \"_\", round(longitude, 6))) %>%\n  select(key, elev_m)\n# one of the values is zero altitude: this is in the ocean (see map)\n\n# Coordinates whose elevation came back as exactly 0\nzero_pts <- elev_tbl %>%\n  filter(!is.na(elev_m), elev_m == 0) %>%\n  transmute(x = round(longitude, 6), y = round(latitude, 6)) %>%\n  distinct()\n```\n\nReplace this longitude and latitude with the average longitude and latitude in the area.\n\n```{r}\n# fixing the incorrect long and lat value\naval %>%\n  filter(round(longitude, 6) == zero_pts$x,\n         round(latitude, 6) == zero_pts$y)\n\n# it's the only observation in this grid\naval %>% \n  filter(OSgrid == \"NN077044\")\n\n# average long and lat in the area:\nmean_coord <- aval %>%\n  filter(Area == \"Creag Meagaidh\") %>%\n  summarise(\n    mean_lon = mean(longitude, na.rm = TRUE),\n    mean_lat = mean(latitude, na.rm = TRUE)\n  )\n\n# known bad coordinates (round to avoid float mismatch)\nbad_lon <- round(as.numeric(zero_pts$x), 6)\nbad_lat <- round(as.numeric(zero_pts$y), 6)\n\n# replacement mean values\nmean_lon <- mean_coord$mean_lon\nmean_lat <- mean_coord$mean_lat\n\n# replace in aval\naval <- aval %>%\n  mutate(\n    longitude = if_else(round(longitude, 6) == bad_lon & round(latitude, 6) == bad_lat,\n                        mean_lon, longitude),\n    latitude  = if_else(round(longitude, 6) == bad_lon & round(latitude, 6) == bad_lat,\n                        mean_lat, latitude)\n  )\n```\n\n\n```{r}\nto_fill <- aval %>%\n  filter(is.na(Alt)) %>%\n  transmute(latitude = latitude, longitude = longitude)\n\nelev_tbl <- oe_lookup(to_fill, batch_size = 80, sleep_secs = 1)\n\n# --- 4) Join back and fill Alt where missing ---\n# 1) Build a lookup key on the API results (rounded to match request precision)\nelev_lu <- elev_tbl %>%\n  mutate(key = paste0(round(latitude, 6), \"_\", round(longitude, 6))) %>%\n  select(key, elev_m)\n\n# 2) Build the same key for your aval rows\naval_key <- paste0(round(aval$latitude, 6), \"_\", round(aval$longitude, 6))\n\n# 3) Match each aval row to the API elevation\nmatch_idx <- match(aval_key, elev_lu$key)\nalt_from_api <- elev_lu$elev_m[match_idx]   # will be NA where no API result\n\n# 4) Replace Alt in place ONLY where it's missing and we have a lookup value\nfill_idx <- is.na(aval$Alt) & !is.na(alt_from_api)\naval$Alt[fill_idx] <- alt_from_api[fill_idx]\n\n# 5) Quick check\ncat(\"Alt missing before fill:\", sum(is.na(aval$Alt)) + sum(fill_idx == TRUE), \"\\n\")\ncat(\"Alt missing after fill :\", sum(is.na(aval$Alt)), \"\\n\")\n```\n\n# Encode angles as circular\n\n```{r}\n# After finishing all angle cleaning:\naval <- aval %>%\n  mutate(\n    Wind.Dir_sin        = if_else(is.na(Wind.Dir), NA_real_, sin(pi * Wind.Dir/180)),\n    Wind.Dir_cos        = if_else(is.na(Wind.Dir), NA_real_, cos(pi * Wind.Dir/180)),\n    Summit.Wind.Dir_sin = if_else(is.na(Summit.Wind.Dir), NA_real_, sin(pi * Summit.Wind.Dir/180)),\n    Summit.Wind.Dir_cos = if_else(is.na(Summit.Wind.Dir), NA_real_, cos(pi * Summit.Wind.Dir/180)),\n    Aspect_sin          = if_else(is.na(Aspect), NA_real_, sin(pi * Aspect/180)),\n    Aspect_cos          = if_else(is.na(Aspect), NA_real_, cos(pi * Aspect/180))\n  )\n```\n\n# Train- Test- Val split\nNote that imputation will be done after this as we will be using bagged trees and want to avoid data leakage. Also note that the split is not random, since this is time series data. Thus the first 70% of the data is the training set.\n\n```{r}\n# STEP 1 — Target + time-based splits ---------------------------------------\nset.seed(7)\n\n# 0) Ensure the columns we need exist & are the right type\nstopifnot(all(c(\"Date\") %in% names(aval)))\n\n# If FAH_ord doesn't exist or isn't correctly ordered, (re)create it from FAH\ntarget_levels <- c(\"Low\",\"Moderate\",\"Considerable -\",\"Considerable +\",\"High\")\nif (!(\"FAH_ord\" %in% names(aval)) ||\n    !is.ordered(aval$FAH_ord) ||\n    !identical(levels(aval$FAH_ord), target_levels)) {\n  stopifnot(\"FAH\" %in% names(aval))\n  aval <- aval %>%\n    mutate(FAH_ord = factor(FAH, levels = target_levels, ordered = TRUE))\n}\n\n# Parse Date robustly if needed\nif (!inherits(aval$Date, \"Date\")) {\n  aval <- aval %>%\n    mutate(Date = as.Date(parse_date_time(\n      Date, orders = c(\"ymd HMS\",\"ymd HM\",\"ymd\",\"dmy HMS\",\"dmy HM\",\"dmy\")\n    )))\n}\n\n# Drop rows with missing target or Date (NNs can't train on NA targets)\naval <- aval %>% filter(!is.na(Date), !is.na(FAH_ord))\n\n# 1) Create time-based splits: 70% train, 15% val, 15% test by calendar time\ncut1 <- quantile(aval$Date, probs = 0.80, na.rm = TRUE, type = 1)\n#cut2 <- quantile(aval$Date, probs = 0.85, na.rm = TRUE, type = 1)\n\ntrain <- aval %>% filter(Date <= cut1)\n#val   <- aval %>% filter(Date >  cut1 & Date <= cut2)\ntest  <- aval %>% filter(Date >  cut1)#cut2)\n\n# Sanity checks: no overlap & correct ordering\n#stopifnot(max(train$Date) < min(val$Date), max(val$Date) < min(test$Date))\n\n# 2) Quick class balance check (important for NN)\ncat(\"Train date range: \", min(train$Date), \"to\", max(train$Date), \"\\n\")\n#cat(\"Val   date range: \", min(val$Date),   \"to\", max(val$Date),   \"\\n\")\ncat(\"Test  date range: \", min(test$Date),  \"to\", max(test$Date),  \"\\n\\n\")\n\ncat(\"Class counts (train):\\n\"); print(table(train$FAH_ord))\ncat(\"\\nClass proportions (train):\\n\"); print(round(prop.table(table(train$FAH_ord)), 3))\n\n#cat(\"\\nClass counts (val):\\n\"); print(table(val$FAH_ord))\ncat(\"\\nClass counts (test):\\n\"); print(table(test$FAH_ord))\n```\nNote the imbalance in FAH_ord. The test set has no High cases. But, the training set has enough data to train on to be able to identify High risk cases.\n\n```{r}\n# extract dates \n\ntrain_dates = train$Date\ntest_dates = test$Date\n```\n\n\n# Pre-processing data\n\n```{r}\n# STEP 2 — NN-ready preprocessing with recipes --------------------------------\n\n# 1) Choose columns to DROP (IDs, raw angles, dates, free text) ---------------\ndrop_cols <- intersect(\n  c(\n    \"FAH\",                # raw target (we'll use FAH_ord)\n    \"Date\", \"DateTime\",   # time stamps (we keep year/month/doy/season)\n    \"Wind.Dir\", \"Summit.Wind.Dir\", \"Aspect\",   # raw angles (keep sin/cos)\n    \"OSgrid\",             # grid ID (too high-cardinality for one-hot)\n    \"Location\"            # often messy/free-text; drop for NN baseline\n  ),\n  names(train)\n)\n\n# 2) Identify informative-missing flags so we can avoid scaling them ----------\nflag_prefixes <- c(\n  \"av_cat_missing\", \"ski_pen_missing\", \"crystals_missing\",\n  \"wetness_missing\", \"snow_index_missing\",\n  \"summit_wind_dir_missing\", \"summit_wind_speed_missing\", \"summit_air_temp_missing\"\n)\nflag_cols <- unique(unlist(lapply(flag_prefixes, function(p) grep(p, names(train), value = TRUE))))\n# You may have named them with `_initial`. If so, they’ll be picked up by grep above.\n\n#flag_cols = c(flag_cols, \"Area\")\n\n# 3) Build the recipe ---------------------------------------------------------\nrec <- recipe(FAH_ord ~ ., data = train) %>% \n  # drop columns we don't want to feed to the NN\n  step_rm(all_of(drop_cols)) %>%\n  # explicitly confirm Area is a predictor (this line is optional, since it’s already a predictor by default)\n  update_role(Area, new_role = \"predictor\") %>%\n  # collapse *very* rare categories to \"other\"\n  step_other(all_nominal_predictors(), threshold = 0.005, other = \"other\") %>% \n  # impute categoricals by mode\n  step_impute_mode(all_nominal_predictors()) %>%\n  # impute numerics by bagged trees\n  step_impute_bag(all_numeric_predictors()) %>%\n  # one-hot encode categoricals\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%\n  # drop zero-variance and near-zero-variance columns\n  step_zv(all_predictors()) %>%\n  step_nzv(all_predictors()) %>%\n  # scale/center numerics (excluding the missingness flags)\n  step_normalize(all_numeric_predictors(), -all_of(c(\"Area_Creag.Meagaidh\",  \"Area_Glencoe\",             \"Area_Lochaber\",\"Area_Northern.Cairngorms\", \"Area_Southern.Cairngorms\" ,\"Area_Torridon\",flag_cols)))\n\n# 4) Prep on TRAIN only, then bake to splits ---------------------------------\nrec_prep <- prep(rec, training = train, retain = TRUE)\n# this calculates the mean/sd for scaling, finds the mode of categoricals, trains the bagged trees for imputation.\n\nx_train <- bake(rec_prep, new_data = train) # apply to train set\n#x_val   <- bake(rec_prep, new_data = val) # apply to val set\nx_test  <- bake(rec_prep, new_data = test) # apply to test set\n\n# 5) Quick sanity checks ------------------------------------------------------\ncat(\"Shapes:\\n\")\ncat(\"  train:\", dim(x_train)[1], \"rows x\", dim(x_train)[2], \"cols\\n\")\n#cat(\"  val  :\", dim(x_val)[1],   \"rows x\", dim(x_val)[2],   \"cols\\n\")\ncat(\"  test :\", dim(x_test)[1],  \"rows x\", dim(x_test)[2],  \"cols\\n\\n\")\n\ncat(\"Any NA left? \",\n    any(vapply(x_train, function(col) any(is.na(col)), logical(1))),\n   # any(vapply(x_val,   function(col) any(is.na(col)), logical(1))),\n    any(vapply(x_test,  function(col) any(is.na(col)), logical(1))), \"\\n\")\n\ncat(\"\\nTarget distribution after preprocessing (train):\\n\")\nprint(table(x_train$FAH_ord))\n\ncat(\"\\nTarget distribution after preprocessing (test):\\n\")\nprint(table(x_test$FAH_ord))\n```\nchange splits to 0.8 train 0.2 test\n\nTalk to data set balance \n\n\n## Changes \n\n- remove unnecessary columns \n- encoded categorical and binary variables \n\n\n```{r}\n\n#encode reponse variable as integers, presevre ordinal property\n\nlev = c(\"Low\", \"Moderate\", \"Considerable-\", \"Considerable+\", \"High\")\n\nX_train = x_train %>% mutate(\n  #FAH_ord = factor(FAH, )\n  FAH_ord= as.integer(FAH_ord) - 1\n  )\n\n# X_val = x_val %>% mutate(\n#   #FAH_ord = factor(FAH, )\n#   FAH_ord= as.integer(FAH_ord) - 1\n#   )\n\nX_test = x_test %>% mutate(\n  #FAH_ord = factor(FAH, )\n  FAH_ord= as.integer(FAH_ord) - 1\n  )\n```\n\n\nremove month and doy column, rebind full dates for sequence generation in python (keep year), as well as missingness cols \n\n```{r}\n\n#remove doy and month , and NOT missingness indicators\n\nX_train = X_train[, -(22:24)]\nX_test = X_test[, -(22:24)]\n\n#rebind dates \n\nX_train = cbind(train_dates, X_train)\nX_test = cbind(test_dates, X_test)\n\n```\n\n\nReplace the season binaries with a doy sin/cos for smooth periodicity\n\n```{r}\nX_train <- X_train %>%\n  mutate(\n    doy = yday(train_dates),                              # 1..365/366\n    period = if_else(leap_year(train_dates), 366, 365),   # handle leap years\n    doy_sin = sin(2*pi * doy / period),\n    doy_cos = cos(2*pi * doy / period)\n  ) %>%\n  select(-doy, -period)  # optional: drop helpers\n\n#remove month binaries \n\nX_train = X_train %>% \n  select(-season_DJF, -season_MAM)\n\nX_test <- X_test %>%\n  mutate(\n    doy = yday(test_dates),                              # 1..365/366\n    period = if_else(leap_year(test_dates), 366, 365),   # handle leap years\n    doy_sin = sin(2*pi * doy / period),\n    doy_cos = cos(2*pi * doy / period)\n  ) %>%\n  select(-doy, -period)  # optional: drop helpers\n\n#remove month binaries \n\nX_test = X_test %>% \n  select(-season_DJF, -season_MAM)\n\n```\n\n\nwrite to csv\n\n```{r}\nwrite.csv(X_train, \"X_train.csv\")\nwrite.csv(X_test, \"X_test.csv\")\n```\n\nwill isolate response and do validation windown splits in python\n\nremove initial missing windows\n\n```{r}\n\nX_train = read.csv(\"X_train.csv\")\n\n```\n\n\n```{python}\nimport random\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader, Subset\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\nimport torch.nn.functional as F\nimport numpy as np\n\n#from google.colab import files\nimport pandas as pd\nimport io\n\nimport pandas as pd # Example library for loading data\n\n#replace with your file path\nfile_path = \"C:/Users/chris/OneDrive/Documents/UNI/STATS/Masters/DS4I/Assignment/Avalanche-Forecast-Prediction/X_train.csv\"\n\n# 2. Check if the path is provided\nif file_path:\n    print(f\"Attempting to load: {file_path}\")\n    \n    # 3. Load the file\n    try:\n        # Note: If the user pastes a Windows path with single backslashes (\\), \n        # Python might need double backslashes (\\\\) or forward slashes (/) to work correctly.\n        # It's safest to manually replace them or ask the user to use forward slashes.\n        train = pd.read_csv(file_path.replace('\\\\', '/')) \n        print(\"File loaded successfully!\")\n        print(train.head())\n    except FileNotFoundError:\n        print(\"Error: The file path was not found.\")\n    except Exception as e:\n        print(f\"Error loading file: {e}\")\nelse:\n    print(\"No path provided.\")\n```\n\n```{python}\n#replace with your file path\nfile_path = \"C:/Users/chris/OneDrive/Documents/UNI/STATS/Masters/DS4I/Assignment/Avalanche-Forecast-Prediction/X_test.csv\"\n\n# 2. Check if the path is provided\nif file_path:\n    print(f\"Attempting to load: {file_path}\")\n    \n    # 3. Load the file\n    try:\n        # Note: If the user pastes a Windows path with single backslashes (\\), \n        # Python might need double backslashes (\\\\) or forward slashes (/) to work correctly.\n        # It's safest to manually replace them or ask the user to use forward slashes.\n        test = pd.read_csv(file_path.replace('\\\\', '/')) \n        print(\"File loaded successfully!\")\n        print(test.head())\n    except FileNotFoundError:\n        print(\"Error: The file path was not found.\")\n    except Exception as e:\n        print(f\"Error loading file: {e}\")\nelse:\n    print(\"No path provided.\")\n```\n\nPrepare Sequences \n\n```{python}\ntarget_col = \"FAH_ord\"\n\ntrain.rename(columns={train.columns[1]: 'Date'}, inplace=True)\ntest.rename(columns={test.columns[1]: 'Date'}, inplace=True)\n\n#remove carried over index column\ntrain.drop(columns=train.columns[0], inplace=True)\ntest.drop(columns=test.columns[0], inplace=True)\n\ndate_col   = \"Date\"\narea_cols  = [c for c in train.columns if c.startswith(\"Area_\")]     # one-hot 0/1\nstatic_num = [c for c in [\"longitude\",\"latitude\",\"Alt\",\"Incline\"] if c in train.columns]\nstatic_cols = static_num + area_cols                                 # fed once per window\n\n# dynamic = numeric columns minus target, static, and labels\ndef numeric_cols(df): return df.select_dtypes(include=[np.number]).columns.tolist()\nban = set([target_col]) | set(static_cols)\ndynamic_cols = [c for c in numeric_cols(train) if c not in ban]\n\ntrain[\"_src\"] = \"train\"; test[\"_src\"] = \"test\"\ndf = pd.concat([train, test], ignore_index=True)\ntest_start_ts = pd.to_datetime(test[date_col]).min()   # Timestamp\nassert pd.notna(test_start_ts), \"Could not determine test start date.\"\n\n# Area id from one-hots\ndf[\"__area_id__\"] = np.argmax(df[area_cols].values, axis=1)\ndf = df.sort_values([\"__area_id__\", date_col]).reset_index(drop=True)\n```\n\nadd in previous forecast to predict next forecast\n\n```{python}\n\n#before building windows:\ndf = df.sort_values([\"__area_id__\", date_col])\ndf[\"FAH_prev\"] = (df.groupby(\"__area_id__\")[target_col]\n                    .shift(1).astype(\"float32\"))\ndf[\"FAH_prev\"] = df[\"FAH_prev\"].fillna(0)   # or area-wise mean\n\n# add to dynamic cols if not there already\nif \"FAH_prev\" not in dynamic_cols:\n    dynamic_cols.append(\"FAH_prev\")\n\n# rebuild windows (same L,H) to get new Xseq_* with the extra column\n\n\n```\n\n```{python}\n\ntrain.head()\ntest.head()\n\n```\n\n```{python}\n\n#before building windows:\ndf = df.sort_values([\"__area_id__\", date_col])\ndf[\"FAH_prev\"] = (df.groupby(\"__area_id__\")[target_col]\n                    .shift(1).astype(\"float32\"))\ndf[\"FAH_prev\"] = df[\"FAH_prev\"].fillna(0)   # or area-wise mean\n\n# add to dynamic cols if not there already\nif \"FAH_prev\" not in dynamic_cols:\n    dynamic_cols.append(\"FAH_prev\")\n\n# rebuild windows (same L,H) to get new Xseq_* with the extra column\n\n```\n\nMakes the sequences for the LSTM, of L length (usually 7, 14)\n\n```{python}\nL, H = 14, 0  # look-back and horizon\ndef build_windows_with_dates(df, L, H, dyn_cols, sta_cols, target_col, date_col):\n    X_seq, X_sta, y, tgt_dates = [], [], [], []\n    for a, g in df.groupby(\"__area_id__\", sort=False):\n        g = g.sort_values(date_col)\n        dyn = g[dyn_cols].to_numpy(np.float32)\n        sta = g[sta_cols].to_numpy(np.float32)\n        yy  = g[target_col].to_numpy(np.int64)\n        dd  = g[date_col].to_numpy('datetime64[ns]')\n        for t in range(L+H, len(g)):\n            X_seq.append(dyn[t-L-H:t-H])\n            X_sta.append(sta[t])\n            y.append(yy[t])\n            tgt_dates.append(dd[t])       # date of the target row\n    return (torch.tensor(np.stack(X_seq)),\n            torch.tensor(np.stack(X_sta)),\n            torch.tensor(np.array(y)),\n            np.array(tgt_dates))\n\nXseq_all, Xsta_all, y_all, tgt_dates = build_windows_with_dates(\n    df, L, H, dynamic_cols, static_cols, target_col, date_col\n)\n\n# 6) SAFE comparison: convert the Timestamp to numpy datetime64 *with unit*\ntest_start_np = np.datetime64(test_start_ts, 'ns')\nis_test  = tgt_dates >= test_start_np\nis_train = ~is_test\n\nXseq_tr, Xsta_tr, y_tr = Xseq_all[is_train], Xsta_all[is_train], y_all[is_train]\nXseq_te, Xsta_te, y_te = Xseq_all[is_test],  Xsta_all[is_test],  y_all[is_test]\n\nprint(\"Train:\", Xseq_tr.shape, Xsta_tr.shape, y_tr.shape)\nprint(\"Test :\", Xseq_te.shape, Xsta_te.shape, y_te.shape)\n```\n\n\nShows the dimensions of the test and train sequences. For train, 8365 sequences, of 14 observations each, of 44 variables. Slightly fewer sequences than there were observations in the train split, as lose some at the bounds where can't build full sequence. \n\n\nExample sequence\n```{python}\nprint(Xseq_tr[[1]])\n```\n\nmake sure no sequences target dates in test window\n\n```{python}\n# You already computed this earlier:\n# Xseq_all, Xsta_all, y_all, tgt_dates = build_windows_with_dates(...)\n\n# And the test boundary:\ntest_start_ts = pd.to_datetime(test[\"Date\"]).min()\ntest_start_np = np.datetime64(test_start_ts, 'ns')\n\n# Mask by each window's TARGET date\nis_test  = tgt_dates >= test_start_np\nis_train = ~is_test\n\n# Training/Testing target dates aligned to y_tr / y_te\ntgt_dates_tr = tgt_dates[is_train]\ntgt_dates_te = tgt_dates[is_test]\n\n# Sanity check\nassert len(tgt_dates_tr) == y_tr.shape[0]\nassert len(tgt_dates_te) == y_te.shape[0]\n```\n\nBreak sequences into folds that follow each other (forward chaining) for time aware validation split\n\neach fold strictly goes past to future, and never randomly shuffles\n\nso for fold k, validation is a contiguous block of unique target dates, and training is alll earlier windows except embargo of L (sequence length) days before validation block, which prevents feature leakage \n\nIs an expanding window so as move forward in time , train includes more history \n\n```{python}\n# ---------- 1) Build forward-chaining folds ----------\ndef make_time_folds(dates, K=5, gap_days=0):\n    \"\"\"Return list of (train_idx, val_idx) respecting time and an optional gap.\"\"\"\n    dates = np.array(dates)\n    uniq = np.unique(dates)\n    # split unique dates into K chronological bins (roughly equal by time span)\n    cuts = np.quantile(np.arange(len(uniq)), np.linspace(0,1,K+1)).round().astype(int)\n    folds = []\n    for k in range(K):\n        start_u, end_u = cuts[k], cuts[k+1]\n        val_days = set(uniq[start_u:end_u])\n        if not val_days:\n            continue\n        val_mask = np.array([d in val_days for d in dates])\n        # embargo/gap: drop train samples within gap_days before the val start\n        if gap_days > 0:\n            val_start = uniq[start_u]\n            gap_mask = dates >= (np.datetime64(val_start) - np.timedelta64(gap_days, 'D'))\n        else:\n            gap_mask = np.zeros_like(val_mask, dtype=bool)\n        train_mask = (~val_mask) & (~gap_mask) & (dates < np.max(list(val_days)))\n        tr_idx = np.where(train_mask)[0]\n        va_idx = np.where(val_mask)[0]\n        if len(tr_idx) and len(va_idx):\n            folds.append((tr_idx, va_idx))\n    return folds\n\n```\n\n## Neural Network architecture\n\nUsing two measures primarly: QWK and CORN\n\nQuadratic Weighted Kappa (QWK) measures agreement between two sets of ordinal ratings (predicted vs forecasted hazard level). Applies quadratic weighting - mistakes that are further apart are more penalised than close ones. lies on -1 to 1\n\nQWK is non-differntiable (can't be used in backprop), so not used in training only as metric\n\nInstead train with differentiable loss - CORN. QWK is used for validation and for early stopping\n\nCumulative Ordinal Regression for NN (CORN). \n\"CORN reframes the multi-class problem into a series of simpler, ordered binary classification tasks.\"\n\nChose CORN over CE because CE treats classes as nominal, so does not distinguish between misclassifications (treats all errors equally bad which is not ideal for ordinal case).\n\nCORN creates K-1 logits for binary question Pr(y>K), as opposed to CE which generates K softmax logits\n\n(note that the loss is on logits vs targets , not using thresholds. after training the logits are concerted to probabilities , which are then put against the thresholds for the binary questions.)\n\nIn the code , pr(y>k) is upweighted for rare hazards - combats imbalance (also have WeightedRandomSampler)\n\nFunctions:\n\ncorn_targets : builds K-1 binary labels for CORN. for each threshold k, target is 1 if y>k else 0\n\ncorn_loss : loss function used in training\n\ncorn_predict_logits_to_labels : converts logits to class predictions \n\n```{python}\nimport torch, torch.nn as nn, torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, Subset, WeightedRandomSampler\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nK = int(y_tr.max().item()) + 1  # e.g., 5 classes (0..4)\n\ndef corn_targets(y, K):\n    ks = torch.arange(K-1, device=y.device).unsqueeze(0).expand(y.size(0), -1)\n    return (y.unsqueeze(1) > ks).float() \n\ndef corn_loss(logits, y, K, pos_weight=None):\n    T = corn_targets(y, K)\n    return F.binary_cross_entropy_with_logits(logits, T, pos_weight=pos_weight)\n\n@torch.no_grad()\ndef corn_predict_logits_to_labels(logits, taus=None):\n    p = torch.sigmoid(logits)            \n    if taus is None:\n        taus = torch.full((logits.size(1),), 0.5, device=logits.device)\n    return (p > taus).sum(dim=1)   \n```\n\n## LSTM\n\n```{python}\n\nclass LSTM(nn.Module):\n  #set up the arch\n  def __init__(self, dynamic_dim , static_dim, output_dim, hidden_dim, num_hidden_layers, rnn_dropout, head_dropout, bidirectional = False):\n    super().__init__()\n   # self.flatten = nn.Flatten()\n\n    self.rnn = nn.LSTM(dynamic_dim, hidden_size=hidden_dim, num_layers=num_hidden_layers, batch_first=True, dropout=(rnn_dropout if num_hidden_layers > 1 else 0.0)\n                       , bidirectional=bidirectional)\n\n  #will have to alter MLP input dim if bidirectional set to True\n\n    self.head = nn.Sequential(\n            nn.Linear(hidden_dim + static_dim, 64),\n            nn.ReLU(), nn.Dropout(head_dropout))#,\n            #nn.Linear(in_features=64, out_features=output_dim))#,\n            #nn.Softmax(dim = 5))\n\n    #self.corn = CORNHead(64, K)\n    #self.K = K\n    \n    # CORN head: K-1 thresholds\n    self.corn = nn.Linear(64, output_dim-1)\n    self.K = K\n\n\n  def forward(self, X_seq, X_static):\n    _, (hiddenStates,_) = self.rnn(X_seq)\n    z = torch.cat((hiddenStates[-1], X_static), dim=1)\n    z = self.head(z)\n    return self.corn(z)\n\n\n```\n\nFold wise imbalance for CORN (fights imbalance in the data). Up weights events that are rare , since our data is imbalanced - higher hazard levels rarer. Calculated as number of responses below K reponse over number above - upweights the rarer classes\nComputes one weight per threshold (response value). Makes rare cases matter more in the BCE loss\n\n```{python}\ndef corn_pos_weight_cb(y_fold, K=5, beta=0.999, device=device):\n    \"\"\"\n    Class-balanced 'effective number' weights for CORN thresholds.\n    For threshold k, positives are (y > k).\n    \"\"\"\n    y_np = y_fold.cpu().numpy()\n    pos = np.array([(y_np > k).sum() for k in range(K-1)], dtype=np.float64)\n    pos = np.clip(pos, 1, None)\n    w = (1 - beta) / (1 - np.power(beta, pos))\n    w = w / w.mean()       # normalize nicely\n    return torch.tensor(w, dtype=torch.float32, device=device)  # (K-1,)\n```\n\nLoads validation fold and evaluates model on it, using ADAM optimiser and CORN for backprop and QWK for validation and early stopping\n\n```{python}\n@torch.no_grad()\ndef eval_corn(model, loader, taus=None):\n    model.eval()\n    ys, ps, loss_sum, n = [], [], 0.0, 0\n    for xseq, xsta, y in loader:\n        xseq, xsta, y = xseq.to(device), xsta.to(device), y.to(device)\n        logits = model(xseq, xsta)\n        loss = corn_loss(logits, y, K)\n        pred  = corn_predict_logits_to_labels(logits, taus=taus.to(device) if taus is not None else None)\n        ys.append(y.cpu().numpy()); ps.append(pred.cpu().numpy())\n        loss_sum += loss.item() * y.size(0); n += y.size(0)\n    y_true = np.concatenate(ys); y_pred = np.concatenate(ps)\n    va_loss = loss_sum / max(n,1)\n    acc = accuracy_score(y_true, y_pred)\n    f1  = f1_score(y_true, y_pred, average=\"macro\")\n    mae = np.abs(y_true - y_pred).mean()\n    try: qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    except: qwk = float(\"nan\")\n    return va_loss, acc, f1, mae, qwk, y_true, y_pred\n\ndef fit_one_corn(model, dl_tr, dl_va, y_tr_fold, lr=2e-3, weight_decay=1e-4,\n                 max_epochs=40, patience=6, head_dropout=0.3):  # patience refers to early stopping epochs\n    model = model.to(device)\n    pos_weight = corn_pos_weight_cb(y_tr_fold, K=K, beta=0.999, device=device)\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    best_state, best_metric, bad = None, -1e9, 0\n    for ep in range(1, max_epochs+1):\n        model.train()\n        for xseq, xsta, y in dl_tr:\n            xseq, xsta, y = xseq.to(device), xsta.to(device), y.to(device)\n            logits = model(xseq, xsta)\n            loss = corn_loss(logits, y, K, pos_weight=pos_weight)\n            opt.zero_grad(); loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n        # validate (no tuned thresholds yet)\n        va_loss, acc, f1, mae, qwk, _, _ = eval_corn(model, dl_va, taus=None)\n        metric = qwk if np.isfinite(qwk) else (acc - mae)  # ordinal-friendly early stop\n        if metric > best_metric + 1e-4:\n            best_metric, bad = metric, 0\n            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n        else:\n            bad += 1\n            if bad >= patience:\n                break\n    if best_state: model.load_state_dict(best_state)\n    return model\n```\n\nDataloaders\n\nweighted random sampler rwweights which samples appear in batches so minority classes are more common. Has the effect of making bayches contain more underrepresented classes (increased attention on rare events). \n\n```{python}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef make_loaders(Xseq, Xsta, y, tr_idx, va_idx, batch_size):\n    ds = TensorDataset(Xseq, Xsta, y)\n    dl_tr = DataLoader(torch.utils.data.Subset(ds, tr_idx), batch_size=batch_size, shuffle=True)\n    dl_va = DataLoader(torch.utils.data.Subset(ds, va_idx), batch_size=batch_size, shuffle=False)\n    return dl_tr, dl_va\n  \ndef make_sampler_from_labels(y_idxed):\n    counts = torch.bincount(y_idxed.cpu(), minlength=K).float().clamp_min(1)\n    cls_w = (len(y_idxed) / (K * counts)).numpy()\n    sample_w = cls_w[y_idxed.cpu().numpy()]\n    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n\n\ndef loaders_for_fold(tr_idx, va_idx, batch_size=256, oversample=True):\n    ds_tr_all = TensorDataset(Xseq_tr, Xsta_tr, y_tr)\n    y_tr_fold = y_tr[tr_idx]\n    if oversample:\n        sampler = make_sampler_from_labels(y_tr_fold)\n        dl_tr = DataLoader(Subset(ds_tr_all, tr_idx), batch_size=batch_size,\n                           sampler=sampler, drop_last=False)\n    else:\n        dl_tr = DataLoader(Subset(ds_tr_all, tr_idx), batch_size=batch_size,\n                           shuffle=True, drop_last=False)\n    dl_va = DataLoader(Subset(ds_tr_all, va_idx), batch_size=batch_size,\n                       shuffle=False, drop_last=False)\n    return dl_tr, dl_va, y_tr_fold\n```\n\n\n## Hyperparameter tuning\n\n\nDon't Run !!!!\n\n```{python}\nrandom.seed(20)\nimport optuna\n\n# Build folds \nfolds = make_time_folds(tgt_dates_tr, K=5, gap_days= 10)#14)\n\ndef objective(trial):\n    # --- sample hparams ---\n    hidden_dim   = trial.suggest_categorical(\"hidden_dim\", [32, 48, 64])\n    num_layers   = trial.suggest_int(\"num_layers\", 1, 2)\n    rnn_dropout  = trial.suggest_float(\"rnn_dropout\", 0.0, 0.5) if num_layers > 1 else 0.0\n    head_dropout = trial.suggest_float(\"head_dropout\", 0.1, 0.5)\n    lr           = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n    batch_size   = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n\n    scores = []\n    for (tr_idx, va_idx) in folds:            # or folds[-1:] if you only want the latest fold\n        dl_tr, dl_va, y_tr_fold = loaders_for_fold(tr_idx, va_idx,\n                                                   batch_size=batch_size,\n                                                   oversample=True)\n\n        model = LSTM(dynamic_dim=Xseq_tr.shape[-1],\n                                static_dim=Xsta_tr.shape[-1],\n                                output_dim=int(y_tr.max().item())+1,\n                                hidden_dim=hidden_dim,\n                                num_hidden_layers=num_layers,\n                                rnn_dropout=rnn_dropout,\n                                head_dropout=head_dropout,\n                                bidirectional=False)\n\n        # ⬇⬇ pass y_tr_fold here\n        model = fit_one_corn(model, dl_tr, dl_va, y_tr_fold,\n                             lr=lr, weight_decay=weight_decay,\n                             max_epochs=30, patience=5)\n\n        _, acc, f1, mae, qwk, *_ = eval_corn(model, dl_va)  # taus=None during tuning\n        scores.append(qwk if np.isfinite(qwk) else (acc - mae))\n\n    return float(np.mean(scores))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50, show_progress_bar=True)\n\nprint(\"Best params:\", study.best_trial.params)\nprint(\"Best CV score (QWK):\", study.best_value)\n\n```\n\nBest params: {'hidden_dim': 32, 'num_layers': 1, 'head_dropout': 0.3472958534673855, 'lr': 0.0028396801498878554, 'weight_decay': 2.2404755878518988e-06, 'batch_size': 256}\nBest CV score (QWK): 0.6344358948490426\n\nTune CORN Thresholds\n\nNow do small grid search using threholds and final val set (should be most similar to test set) to get final thresholds. Use hyperparameters from early CV to make final model , which will make logits and then probs, but those probs will be converted to classes using these threholds from final val split\n\nbecause test period mix is different (many more 0s), tune the final threholds for the loigit -> predictions step on final val slice , in hopes it most closely resembles the test set\n\n```{python}\nrandom.seed(20)\n#best = study.best_trial.params \n\nbest = {\n  \"lr\": 0.0028396801498878554,\n  \"hidden_dim\": 32,\n  \"num_layers\": 1,\n  \"head_dropout\": 0.3472958534673855,\n  \"weight_decay\": 2.2404755878518988e-06,\n  \"batch_size\": 256\n}\n\n# pick the last fold (most recent dates in train set)\ntr_idx, va_idx = folds[-1]       \n\n# loaders with oversampling - oversample rare hazards\nbatch_size = 256\ndl_tr, dl_va, y_tr_fold = loaders_for_fold(tr_idx, va_idx, batch_size=batch_size, oversample=True)\n\n# fit model\ndyn_dim = Xseq_tr.shape[-1]; sta_dim = Xsta_tr.shape[-1]\nmodel = LSTM(dyn_dim, sta_dim, output_dim=K, hidden_dim=48, num_hidden_layers=best[\"num_layers\"], rnn_dropout= 0, head_dropout=best[\"head_dropout\"]) #best[\"rnn_dropout\"]\n\n# train on latest fold\nmodel = fit_one_corn(model, dl_tr, dl_va, y_tr_fold, lr= best[\"lr\"], weight_decay=best[\"weight_decay\"], max_epochs=50, patience=8)\n\n# collect validation probabilities\n@torch.no_grad()\ndef collect_probs_and_labels(model, loader):\n    model.eval()\n    probs, ys = [], []\n    for xseq, xsta, y in loader:\n        xseq, xsta = xseq.to(device), xsta.to(device)\n        logits = model(xseq, xsta)\n        probs.append(torch.sigmoid(logits).cpu().numpy())  # (B, K-1)\n        ys.append(y.cpu().numpy())\n    return np.vstack(probs), np.concatenate(ys)\n\nprobs_val, y_val = collect_probs_and_labels(model, dl_va)\n```\n\nTune monotone thresholds on latest fold \nthe threholds are monotone - meaning that for the lower classes they must be greater than or equal to those decision boundaries/threholds for the higher classes (so higher classes aren't easier to cross than lower ones)\n\n```{python}\nrandom.seed(20)\nfrom sklearn.metrics import cohen_kappa_score\n\ndef tune_corn_thresholds(probs_val, y_val, grid=np.linspace(0.3, 0.7, 21)):\n    Km1 = probs_val.shape[1]\n    taus = [0.5]*Km1\n    \n    for k in range(Km1):\n        best_tau, best_q = taus[k], -1\n        for t in grid:\n            cand = taus.copy()\n            cand[k] = float(t)\n          \n            for j in range(Km1-2, -1, -1):\n                cand[j] = max(cand[j], cand[j+1])\n            y_pred = (probs_val > np.array(cand)).sum(axis=1)\n            q = cohen_kappa_score(y_val, y_pred, weights=\"quadratic\")\n            if q > best_q:\n                best_q, best_tau = q, cand[k]\n        taus[k] = best_tau\n    taus = np.array(taus, dtype=np.float32)\n    final_q = cohen_kappa_score(y_val, (probs_val > taus).sum(axis=1), weights=\"quadratic\")\n    return taus, final_q\n\ntaus, q_val = tune_corn_thresholds(probs_val, y_val)\nprint(\"Tuned taus (monotone):\", np.round(taus, 3), \" | Val QWK with taus:\", round(q_val, 3))\n\n```\n\nOutput here are the thresholds used for the different classes\n\nTest set evaluation with thresholds (for converting logits to labels 0..4 from final fold - more representative of test set)\n\n```{python}\nrandom.seed(20)\n# Retrain on *all* training windows (use the same hparams you selected on the latest fold)\nds_all = TensorDataset(Xseq_tr, Xsta_tr, y_tr)\n# Oversample for all-train too:\nsampler_all = make_sampler_from_labels(y_tr)\ndl_all = DataLoader(ds_all, batch_size=batch_size, sampler=sampler_all, drop_last=False)\n\n# simple early stopping on the last 10% time slice of train to avoid overfit\nN = Xseq_tr.size(0); split = int(N*0.9)\ndl_tr_full = DataLoader(TensorDataset(Xseq_tr[:split], Xsta_tr[:split], y_tr[:split]),\n                        batch_size=batch_size, sampler=make_sampler_from_labels(y_tr[:split]))\ndl_va_full = DataLoader(TensorDataset(Xseq_tr[split:], Xsta_tr[split:], y_tr[split:]),\n                        batch_size=batch_size, shuffle=False)\n\n#do final model fit with best parameters, with small validation tail to detect overfitting\nmodel_full = LSTM(dyn_dim, sta_dim, output_dim=K, hidden_dim=48, num_hidden_layers=1, rnn_dropout=0, head_dropout=best[\"head_dropout\"]) #best[\"rnn_dropout\"]\nmodel_full = fit_one_corn(model_full, dl_tr_full, dl_va_full, y_tr[:split],\n                          lr=best[\"lr\"], weight_decay=best[\"weight_decay\"], max_epochs=50, patience=8)\n\n# Evaluate Test using the thresholds (taus) we found on the last fold - most similar to test set\ndl_te = DataLoader(TensorDataset(Xseq_te, Xsta_te, y_te), batch_size=batch_size, shuffle=False)\n\n@torch.no_grad()\ndef eval_test_with_taus(model, loader, taus_np):\n    taus_t = torch.tensor(taus_np, dtype=torch.float32, device=device)\n    model.eval()\n    ys, ps, loss_sum, n = [], [], 0.0, 0\n    for xseq, xsta, y in loader:\n        xseq, xsta, y = xseq.to(device), xsta.to(device), y.to(device)\n        logits = model(xseq, xsta)\n        loss = corn_loss(logits, y, K)\n        pred  = corn_predict_logits_to_labels(logits, taus=taus_t)\n        ys.append(y.cpu().numpy()); ps.append(pred.cpu().numpy())\n        loss_sum += loss.item() * y.size(0); n += y.size(0)\n    y_true = np.concatenate(ys); y_pred = np.concatenate(ps)\n    te_loss = loss_sum / max(n,1)\n    acc = accuracy_score(y_true, y_pred)\n    f1  = f1_score(y_true, y_pred, average=\"macro\")\n    mae = np.abs(y_true - y_pred).mean()\n    try: qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    except: qwk = float(\"nan\")\n    return te_loss, acc, f1, mae, qwk\n\nte_loss, te_acc, te_f1, te_mae, te_qwk = eval_test_with_taus(model_full, dl_te, taus)\nprint(f\"TEST | loss {te_loss:.3f} acc {te_acc:.3f} f1 {te_f1:.3f} mae {te_mae:.3f} qwk {te_qwk:.3f}\")\n\n```\nTEST | loss 0.266 acc 0.624 f1 0.326 mae 0.442 qwk 0.482\n\nhigher QWK with similar or lower MAE, better calibrated, fewer big jumps\n\nConfusion Matrix\n\n```{python}\nrandom.seed(20)\nimport torch, numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score\n\nK = 5  # classes 0..4\n\n# collect logits on test\nmodel.eval()\nlogits_list, y_list = [], []\nwith torch.no_grad():\n    for xseq, xsta, y in dl_te:  # your test DataLoader\n        xseq, xsta = xseq.to(device), xsta.to(device)\n        logits = model(xseq, xsta)           # (B, K-1)\n        logits_list.append(logits.cpu())\n        y_list.append(y.cpu())\ny_true    = torch.cat(y_list).numpy()\n\n# after inference on test\nwith torch.no_grad():\n    logits_te = torch.cat([model(x.to(device), s.to(device)).cpu()\n                           for x,s,_ in dl_te])\nprobs_te = torch.sigmoid(logits_te).numpy()     # (N, K-1)\n\ntaus = np.array([0.52, 0.50, 0.50, 0.48], dtype=np.float32)   # from your tuning\ny_pred = (probs_te > taus).sum(axis=1)          # <- use taus here\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score\n\nlabels = [0,1,2,3,4]  # FAH levels\n\ncm = confusion_matrix(y_true, y_pred, labels=labels)\nprint(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n\n# Per-class precision/recall/F1 and macro/weighted averages:\nprint(\"\\nPer-class report:\")\nprint(classification_report(y_true, y_pred, labels=labels, digits=3))\n\n# If you just want the recall vector (per class):\nrecall_per_class = recall_score(y_true, y_pred, labels=labels, average=None)\nprint(\"Recall per class:\", recall_per_class)\n```\n\n\n# Majority Baseline\n\npredicts most frequent class from training set\nhave a global prediction (most common hazard forecast over whole set vs most common per area)\n\n```{python}\nrandom.seed(20)\nimport numpy as np, pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n\n# Global majority from TRAIN labels\nmajor = train[\"FAH_ord\"].mode().iloc[0]\ny_pred_major = np.full(len(test), major)\n\n# Per-Area majority (a stronger baseline)\narea_cols = [c for c in train.columns if c.startswith(\"Area_\")]\ndef area_name(df):\n    idx = df[area_cols].values.argmax(axis=1)\n    return np.array([area_cols[i].replace(\"Area_\",\"\") for i in idx])\n\ntrain[\"AreaName\"] = area_name(train)\ntest[\"AreaName\"]  = area_name(test)\nper_area_major = (train.groupby(\"AreaName\")[\"FAH_ord\"]\n                        .agg(lambda s: s.mode().iloc[0]))\ny_pred_area_major = test[\"AreaName\"].map(per_area_major).fillna(major).to_numpy()\n\ndef report(y_true, y_pred, label):\n    acc  = accuracy_score(y_true, y_pred)\n    f1   = f1_score(y_true, y_pred, average=\"macro\")\n    mae  = np.abs(y_true - y_pred).mean()\n    qwk  = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    print(f\"{label:18s} | acc {acc:.3f}  macroF1 {f1:.3f}  MAE {mae:.3f}  QWK {qwk:.3f}\")\n\nreport(test[\"FAH_ord\"].to_numpy(), y_pred_major,      \"Majority (global)\")\nreport(test[\"FAH_ord\"].to_numpy(), y_pred_area_major, \"Majority (per-area)\")\n```\n\nMajority (global)  | acc 0.305  macroF1 0.094  MAE 0.735  QWK 0.000\nMajority (per-area) | acc 0.393  macroF1 0.200  MAE 0.755  QWK 0.090\n\n\n# Persistence Baseline \n\nPredicts todays hazard as the same as previous day\n\n```{python}\nrandom.seed(20)\n# Build previous-day target per (Area, Date)\ntest_sorted = t#est.sort_values([\"AreaName\",\"Date\"]).copy()\ntrain_sorted= train.sort_values([\"AreaName\",\"Date\"]).copy()\n\n# concat to allow the first test day to look back into the end of train\nall_ = pd.concat([train_sorted, test_sorted], ignore_index=True)\n\n# y_prev(t) = y(t-1) within the same Area\nall_[\"y_prev\"] = (all_.groupby(\"AreaName\")[\"FAH_ord\"]\n                     .shift(1))  # NaN for each area's first row\n\n# Extract predictions for test rows only\npersist_pred = all_.loc[all_.index.isin(test_sorted.index), \"y_prev\"].to_numpy()\n\n# Fallback for the very first day of each area in test (no previous)\n# use that area's majority or global majority\nmask = np.isnan(persist_pred)\nfallback = test_sorted.loc[mask, \"AreaName\"].map(per_area_major).fillna(major).to_numpy()\npersist_pred[mask] = fallback\n\nreport(test_sorted[\"FAH_ord\"].to_numpy(), persist_pred.astype(int), \"Persistence (y[t-1])\")\n```\n\nPersistence (y[t-1]) | acc 0.203  macroF1 0.139  MAE 1.453  QWK -0.041\n\nDiscrepancy between the majority and persistence baselines, and the model's performnace suggests it is better than a naive approach (is learning)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"data_prep_eda.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":"cosmo","math":"mathjax","title":"Data Preparation and EDA"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"data_prep_eda.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"title":"Data Preparation and EDA"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}